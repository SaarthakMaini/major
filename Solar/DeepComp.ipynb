{"cells":[{"cell_type":"code","execution_count":1,"metadata":{"id":"dvcVyqsalxsD"},"outputs":[],"source":["import pandas as pd\n","import numpy as np\n","import torch\n","import torch.nn as nn\n","import torch.nn.functional as F"]},{"cell_type":"code","execution_count":2,"metadata":{"id":"b15yQ0ACl3yY"},"outputs":[],"source":["# Set the base name for the datasets and the directory path\n","RE = \"Solar_PBE\"  # Base name for the dataset\n","address = \"../data/\"  # Directory path where data files are stored\n","\n","# Load training data from CSV files and concatenate them\n","data_train_csv1 = pd.read_csv(address + RE + '_16.csv', index_col=0)  # Load first part of training data\n","data_train_csv2 = pd.read_csv(address + RE + '_17.csv', index_col=0)  # Load second part of training data\n","data_train_csv = pd.concat([data_train_csv1, data_train_csv2])  # Concatenate both training data frames\n","\n","# Load validation and test data from CSV files\n","data_val_csv = pd.read_csv(address + RE + '_18.csv', index_col=0)  # Load validation data\n","data_test_csv = pd.read_csv(address + RE + '_19.csv', index_col=0)  # Load test data\n","\n","# Load price data from CSV file containing imbalance prices\n","data_price = pd.read_csv(address + 'Price_Elia_Imbalance_16_19.csv', index_col=0)  # Load price data\n","\n","# Assign positive imbalance prices to the corresponding datasets\n","data_train_csv['Price(€)'] = data_price['Positive imbalance price'][:len(data_train_csv)]  # Add price column to training data\n","data_val_csv['Price(€)'] = data_price['Positive imbalance price'][len(data_train_csv):len(data_train_csv) + len(data_val_csv)]  # Add price column to validation data\n","data_test_csv['Price(€)'] = data_price['Positive imbalance price'][len(data_train_csv) + len(data_val_csv):]  # Add price column to test data\n"]},{"cell_type":"code","execution_count":3,"metadata":{"id":"jB4phR_Vl4Xg"},"outputs":[],"source":["# Define the battery size and unit for aggregation\n","Battery_Size = 0.15 \n","unit         = 1 \n","\n","# Determine the maximum capacities and prices from the datasets\n","RE_Capacity1 = max(data_train_csv['Power(MW)'])  # Max power for training data\n","RE_Capacity2 = max(data_val_csv['Power(MW)'])    # Max power for validation data\n","RE_Capacity3 = max(data_test_csv['Power(MW)'])   # Max power for test data\n","max_price = max(data_price['Marginal incremental price'])  # Max price across all data\n","\n","# Calculate the sizes of the datasets based on the defined unit\n","size_train0 = int(len(data_train_csv) / unit)\n","size_val0   = int(len(data_val_csv) / unit)\n","size_test0  = int(len(data_test_csv) / unit)\n","\n","# Initialize lists to hold processed training data\n","data_train0 = []  \n","data_train = []  \n","price_train0 = []  \n","price_train = []\n","\n","# Process training data\n","for i in range(size_train0):\n","    # Calculate mean normalized power and price for each unit segment\n","    data_train0.append(round(pd.Series.mean(data_train_csv['Power(MW)'][i*unit:(i+1)*unit]) / RE_Capacity1, 3))\n","    price_train0.append(round(pd.Series.mean(data_train_csv['Price(€)'][i*unit:(i+1)*unit]) / max_price, 3))\n","    # Filter out zero or negative values\n","    if data_train0[i] > 0:\n","        data_train.append(data_train0[i])\n","        price_train.append(price_train0[i])\n","\n","# Initialize lists for validation data\n","data_val0 = []  \n","data_val = []  \n","price_val0 = []  \n","price_val = []\n","\n","# Process validation data\n","for i in range(size_val0):\n","    data_val0.append(round(pd.Series.mean(data_val_csv['Power(MW)'][i*unit:(i+1)*unit]) / RE_Capacity2, 3))\n","    price_val0.append(round(pd.Series.mean(data_val_csv['Price(€)'][i*unit:(i+1)*unit]) / max_price, 3))\n","    if data_val0[i] > 0:\n","        data_val.append(data_val0[i])\n","        price_val.append(price_val0[i])\n","\n","# Initialize lists for test data\n","data_test0 = []  \n","data_test = []  \n","price_test0 = []  \n","price_test = []\n","\n","# Process test data\n","for i in range(size_test0):\n","    data_test0.append(round(pd.Series.mean(data_test_csv['Power(MW)'][i*unit:(i+1)*unit]) / RE_Capacity3, 3))\n","    price_test0.append(round(pd.Series.mean(data_test_csv['Price(€)'][i*unit:(i+1)*unit]) / max_price, 3))\n","    if data_test0[i] > 0:\n","        data_test.append(data_test0[i])\n","        price_test.append(price_test0[i])\n"]},{"cell_type":"code","execution_count":4,"metadata":{"id":"rr6hJBuvl5XY"},"outputs":[],"source":["# PPO Agent (Partially Observable State, Continuous Action Space)\n","# Assumption 1: Standard deviation is fixed\n","# Assumption 2: History is composed of observations only\n"," \n","n_layers         = 2              # Number of LSTM layers in the network\n","in_size          = 2              # Number of input features for the model\n","hidden_size      = 64             # Number of hidden units in each LSTM layer\n","out_size         = 1              # Number of actions (output dimension)\n","T_horizon        = 128            # Time horizon for the PPO algorithm (number of time steps)\n","learning_rate    = 0.001          # Learning rate for the optimizer (controls step size)\n","K_epoch          = 3              # Number of epochs to update the model per batch\n","gamma            = 0.99           # Discount factor for future rewards (values future rewards)\n","lmbda            = 0.95           # Lambda parameter for Generalized Advantage Estimation (controls bias-variance tradeoff)\n","eps_clip         = 0.01           # Clipping parameter for the PPO algorithm (limits the size of policy updates)\n","C_value          = 1              # Coefficient for the critic loss in the overall loss function\n","var              = 0.1**2         # Variance for the action distribution (controls exploration)\n","\n","# Define the LSTM network architecture\n","class LSTM(nn.Module):\n","    def __init__(self):\n","        super(LSTM, self).__init__()\n","        self.fc_s  = nn.Linear(in_size, hidden_size)  # Fully connected layer for input state to hidden\n","        self.rnn   = nn.LSTM(hidden_size, hidden_size, n_layers, batch_first=True)  # LSTM layer\n","        self.fc_pi = nn.Linear(hidden_size, out_size)  # Fully connected layer for action policy output\n","        self.fc_v  = nn.Linear(hidden_size, 1)         # Fully connected layer for value function output\n"," \n","    # Policy function to calculate the action distribution\n","    def pi(self, x, hidden):\n","        x = F.relu(self.fc_s(x))                      # Apply activation function to hidden layer output\n","        x = x.view(1, -1, hidden_size)                # Reshape for LSTM input\n","        x, hidden = self.rnn(x, hidden)               # Pass through the LSTM layer\n","        pi = self.fc_pi(x)                            # Get action probabilities\n","        pi = pi.view(-1, out_size)                   # Reshape action output\n","        return pi, hidden                             # Return action probabilities and hidden state\n","    \n","    # Value function to estimate the expected future rewards\n","    def v(self, x, hidden):\n","        x = F.relu(self.fc_s(x))                      # Apply activation function to hidden layer output\n","        x = x.view(1, -1, hidden_size)                # Reshape for LSTM input\n","        x, hidden = self.rnn(x, hidden)               # Pass through the LSTM layer\n","        v = self.fc_v(x)                              # Get value estimates\n","        v = v.view(-1, 1)                             # Reshape value output\n","        return v                                      # Return estimated value\n"," \n","# Function to train the PPO network\n","def train_net(model, batch, optimizer):\n","    # Initialize lists for storing observations, actions, rewards, etc.\n","    o, H, a, r, o_prime, H_prime, done = [], [], [], [], [], [], []\n","    \n","    # Process each transition in the first batch\n","    for transition in batch[0]:\n","        o.append(transition[0])                    # Append observation to list\n","        a.append(transition[1])                    # Append action to list\n","        r.append([transition[2]])                  # Append reward to list\n","        o_prime.append(transition[3])              # Append next observation to list\n","        done.append([0]) if transition[4] else done.append([1])  # Append done flag (0: not done, 1: done)\n","    \n","    # Process the history transitions\n","    for transition in batch[1]:\n","        H.append(transition[0])                    # Append current hidden state to list\n","        H_prime.append(transition[1])              # Append next hidden state to list\n","        \n","    # Convert lists to PyTorch tensors\n","    o         = torch.tensor(o, dtype=torch.float)  # Convert observations to tensor\n","    H         = (H[0][0].detach(), H[0][1].detach()) # Detach hidden states from graph\n","    a         = torch.tensor(a, dtype=torch.float)  # Convert actions to tensor\n","    r         = torch.tensor(r, dtype=torch.float)  # Convert rewards to tensor\n","    o_prime   = torch.tensor(o_prime, dtype=torch.float)  # Convert next observations to tensor\n","    H_prime   = (H_prime[0][0].detach(), H_prime[0][1].detach()) # Detach next hidden states\n","    done      = torch.tensor(done)                   # Convert done flags to tensor\n"," \n","    # Calculate the old probability distribution of actions\n","    pdf_old = torch.distributions.MultivariateNormal(model.pi(o, H)[0], var * torch.eye(out_size))\n","    prob_old = torch.exp(pdf_old.log_prob(a)).view(len(a), 1)  # Calculate old action probabilities\n","    prob_old = prob_old.detach()                          # Detach from computation graph\n"," \n","    # Calculate target values for the critic\n","    v_target = r + gamma * model.v(o_prime, H_prime) * done\n","    td = r + gamma * model.v(o_prime, H_prime) * done - model.v(o, H)  # Temporal Difference error\n","    td = td.detach().numpy()                             # Detach and convert to numpy for advantage calculation\n","    advantage = []\n","    A = 0.0\n","    # Calculate advantage estimates using GAE\n","    for delta in td[::-1].flatten():\n","        A = delta + gamma * lmbda * A                # Update advantage based on TD error\n","        advantage.append([A])\n","    advantage.reverse()                                 # Reverse to maintain correct order\n","    advantage = torch.tensor(advantage, dtype=torch.float)  # Convert advantage to tensor\n","    \n","    # Update the policy and value networks\n","    for i in range(K_epoch):\n","        pdf = torch.distributions.MultivariateNormal(model.pi(o, H)[0], var * torch.eye(out_size))\n","        prob = torch.exp(pdf.log_prob(a)).view(len(a), 1)  # Calculate current action probabilities\n","        ratio = torch.exp(torch.log(prob) - torch.log(prob_old))  # Calculate probability ratio\n"," \n","        # Calculate loss for the actor (policy) and critic (value function)\n","        loss_actor = torch.min(ratio * advantage, torch.clamp(ratio, 1 - eps_clip, 1 + eps_clip) * advantage)\n","        loss_critic = F.mse_loss(model.v(o, H), v_target.detach())  # Mean squared error loss for critic\n","        loss = -(loss_actor - C_value * loss_critic)                 # Overall loss (actor + critic)\n","        \n","        optimizer.zero_grad()                                     # Reset gradients\n","        loss.mean().backward(retain_graph=True)                  # Backpropagate loss\n","        optimizer.step()                                         # Update model parameters\n"]},{"cell_type":"code","execution_count":5,"metadata":{"id":"9LdaWXmdl6GY"},"outputs":[],"source":["# Environment parameters\n","E_max   = Battery_Size          # Maximum energy capacity of the battery\n","P_max   = E_max                 # Maximum power output, equal to maximum energy capacity\n","tdelta  = unit / 4              # Time delta for the simulation, adjusted based on the unit\n","soc_min = 0.1                   # Minimum state of charge for the battery\n","soc_max = 0.9                   # Maximum state of charge for the battery\n","\n","# Coefficients for battery model parameters\n","a0 = -1.031\n","a1 = 35\n","a2 = 3.685\n","a3 = 0.2156\n","a4 = 0.1178\n","a5 = 0.3201\n","\n","b0 = 0.1463\n","b1 = 30.27\n","b2 = 0.1037\n","b3 = 0.0584\n","b4 = 0.1747\n","b5 = 0.1288\n","\n","c0 = 0.1063\n","c1 = 62.49\n","c2 = 0.0437\n","\n","d0 = 0.0712\n","d1 = 61.4\n","d2 = 0.0288\n","\n","N = 130 * 215 * E_max / 0.1  # Normalization factor based on battery capacity\n","beta = 10 / max_price        # Scaling factor for price normalization\n","\n","class Env():\n","    def __init__(self, data, price):\n","        self.data = data                # Input data for the environment\n","        self.price = price              # Price data associated with the environment\n","        self.state = []                 # Initialize the state of the environment\n","\n","    def reset(self):\n","        gen = self.data[0]             # Initialize generator output from the data\n","        E = E_max / 2                  # Start with half of the maximum energy capacity\n","        state = [[gen, E]]             # Set initial state with generator output and energy\n","        self.state = state              # Store the current state\n","        return state                    # Return the initial state\n","\n","    def step(self, action):\n","        gen = self.data[len(self.state)]  # Get the generator output for the current step\n","        bid = action[0]                    # Get the bid from the action\n","        imb = self.price[len(self.state)]  # Get the imbalance price for the current step\n","\n","        E = self.state[-1][-1]             # Retrieve the current energy level\n","        soc = E / E_max                     # Calculate the state of charge\n","        # Calculate battery voltage based on state of charge\n","        Voc = a0 * np.exp(-a1 * soc) + a2 + a3 * soc - a4 * soc**2 + a5 * soc**3\n","        # Calculate battery series resistance based on state of charge\n","        Rs  = b0 * np.exp(-b1 * soc) + b2 + b3 * soc - b4 * soc**2 + b5 * soc**3\n","        # Calculate total resistance (thermal and other losses)\n","        Rts = c0 * np.exp(-c1 * soc) + c2\n","        Rtl = d0 * np.exp(-d1 * soc) + d2\n","        R   = Rs + Rts + Rtl              # Total resistance\n","\n","        # Calculate maximum charging current based on available energy\n","        I_cmax = 1000000 * (E_max * soc_max - E) / N / (Voc * tdelta)\n","        # Calculate maximum discharging current based on remaining energy\n","        I_dmax = 1000000 * (E - E_max * soc_min) / N / (Voc * tdelta)\n","        # Calculate power available for charging\n","        p_cmax = N * (Voc * I_cmax + I_cmax**2 * R)\n","        # Calculate power available for discharging\n","        p_dmax = N * (Voc * I_dmax - I_dmax**2 * R)\n","\n","        # Limit the charging and discharging power\n","        P_cmax = p_cmax / 1000000\n","        P_dmax = p_dmax / 1000000\n","        P_c = min(max(gen - bid, 0), P_max, P_cmax)  # Charging power\n","        P_d = min(max(bid - gen, 0), P_max, P_dmax)  # Discharging power\n","        p_c = 1000000 * P_c / N\n","        p_d = 1000000 * P_d / N\n","\n","        # Calculate charging current based on voltage and resistance\n","        I_c = -(Voc - np.sqrt(Voc**2 + 4 * R * p_c)) / (2 * R)\n","        # Calculate discharging current based on voltage and resistance\n","        I_d = (Voc - np.sqrt(Voc**2 - 4 * R * p_d)) / (2 * R)\n","\n","        # Determine efficiency and update energy state based on action taken\n","        if not np.isclose(p_c, 0):\n","            eff_c = (Voc * I_c) / p_c  # Efficiency during charging\n","            eff_d = 1                   # Efficiency during discharging assumed to be 1\n","            E_prime = E + eff_c * P_c * tdelta  # Update energy after charging\n","            disp = gen - P_c  # Dispatch during charging\n","        elif not np.isclose(p_d, 0):\n","            eff_d = p_d / (Voc * I_d)  # Efficiency during discharging\n","            eff_c = 1                   # Efficiency during charging assumed to be 1\n","            E_prime = E - (1 / eff_d) * P_d * tdelta  # Update energy after discharging\n","            disp = gen + P_d  # Dispatch during discharging\n","        else:\n","            eff_c = 1  # No charging\n","            eff_d = 1  # No discharging\n","            E_prime = E  # Energy remains the same\n","            disp = gen  # Dispatch is equal to generator output\n","\n","        error = bid - disp  # Calculate the error between the bid and the dispatched power\n","        error_function = abs(error) + beta * P_c + beta * P_d  # Calculate error function\n","        revenue = (imb * disp - imb * abs(bid - disp) - beta * (P_c + P_d)) * tdelta  # Calculate revenue\n","\n","        next_state = self.state + [[gen, E_prime]]  # Update state with new generator output and energy\n","        reward = -error_function  # Reward is the negative of the error function\n","        done = False  # The episode is not done\n","        info = [gen, bid, disp, revenue]  # Additional information returned for analysis\n","\n","        self.state = next_state  # Update the current state\n","        return next_state, reward, done, info  # Return the next state, reward, done flag, and additional info\n"]},{"cell_type":"code","execution_count":6,"metadata":{"id":"Zuc-HkHnl605"},"outputs":[],"source":["# Number of episodes for training the PPO agent\n","total_episode = 500\n","\n","# Calculate maximum iterations per episode based on training data size and time horizon\n","max_iteration = int(len(data_train) / T_horizon)\n","\n","# Print output interval for monitoring training progress\n","print_interval = 1\n","\n","# Initialize the LSTM model for the PPO agent\n","model = LSTM()\n","\n","# Create environment instances for training, validation, and testing\n","env_train = Env(data_train, price_train)  # Training environment with training data and prices\n","env_val   = Env(data_val, price_val)      # Validation environment with validation data and prices\n","env_test  = Env(data_test, price_test)    # Testing environment with testing data and prices\n","\n","# Initialize lists to store bidding values for each phase\n","bid_train, bid_val, bid_test = [], [], []  # Bidding values for training, validation, and testing\n","\n","# Initialize lists to track performance metrics during training and evaluation\n","mae_train, mae_val, mae_test = [], [], []   # Mean Absolute Error for training, validation, and testing\n","mbe_train, mbe_val, mbe_test = [], [], []   # Mean Bidding Error for training, validation, and testing\n","rev_train, rev_val, rev_test = [], [], []    # Revenue for training, validation, and testing\n"]},{"cell_type":"code","execution_count":8,"metadata":{"id":"ZOhpOvKRl7mo"},"outputs":[],"source":["# Initialize the Adam optimizer for the model with the specified learning rate\n","optimizer = torch.optim.Adam(model.parameters(), lr=learning_rate)\n","\n","# Loop over the specified number of episodes for training\n","for n_epi in range(total_episode):\n","    # Initialize lists for bidding values and performance metrics for each episode\n","    bid_train += [[]]; bid_val += [[]]; bid_test += [[]]\n","    mae_train += [[]]; mae_val += [[]]; mae_test += [[]]\n","    mbe_train += [[]]; mbe_val += [[]]; mbe_test += [[]]\n","    rev_train += [[]]; rev_val += [[]]; rev_test += [[]]\n","\n","    # Reset the training environment to get the initial state\n","    state = env_train.reset()\n","\n","    # Initialize hidden states for the LSTM model\n","    history = (torch.zeros([n_layers, 1, hidden_size], dtype=torch.float), \n","               torch.zeros([n_layers, 1, hidden_size], dtype=torch.float))\n","\n","    # Iterate through the maximum number of iterations per episode\n","    for i in range(max_iteration):\n","        batch = [[], []]  # Prepare a batch for storing experience tuples\n","\n","        # Collect actions and rewards over the time horizon\n","        for t in range(T_horizon):\n","            # Get action probabilities and the next hidden state from the model\n","            pi_out, next_history = model.pi(torch.tensor(state[-1], dtype=torch.float), history)\n","\n","            # Sample an action from the probability distribution\n","            action = np.random.multivariate_normal(pi_out.detach().numpy()[0], \n","                                                   var * np.identity(out_size), 1)[0].tolist()\n","\n","            # Take a step in the environment using the selected action\n","            next_state, reward, done, info = env_train.step(action)\n","\n","            # Append the experience tuple to the batch\n","            batch[0].append((state[-1], action, reward, next_state[-1], done))\n","            batch[1].append((history, next_history))\n","\n","            # Update the state and history for the next iteration\n","            state = next_state[:]\n","            history = next_history\n","\n","            # Extract relevant information from the environment feedback\n","            gen = info[0]; bid = info[1]; disp = info[2]; revenue = info[3]\n","\n","            # Store bidding values and performance metrics\n","            bid_train[n_epi] += [bid]\n","            mae_train[n_epi] += [abs(gen - bid)]\n","            mbe_train[n_epi] += [abs(disp - bid)]\n","            rev_train[n_epi] += [revenue]\n","\n","            # Break the loop if the episode is done\n","            if done:\n","                break\n","        \n","        # Train the model using the batch collected\n","        if n_epi != 0:\n","            train_net(model, batch, optimizer)\n","        if done:\n","            break\n","    \n","    # Validation Phase\n","    state = env_val.reset()  # Reset the validation environment\n","    history = (torch.zeros([n_layers, 1, hidden_size], dtype=torch.float), \n","               torch.zeros([n_layers, 1, hidden_size], dtype=torch.float))\n","    \n","    # Iterate through the validation data\n","    for k in range(len(env_val.data) - 1):\n","        pi_out, next_history = model.pi(torch.tensor(state[-1], dtype=torch.float), history)\n","        action = pi_out[0].tolist()  # Get action from the model\n","\n","        # Step the validation environment\n","        next_state, reward, done, info = env_val.step(action)\n","\n","        # Update state and history\n","        state = next_state[:]\n","        history = next_history\n","        \n","        # Record information for validation metrics\n","        gen = info[0]; bid = info[1]; disp = info[2]; revenue = info[3]\n","        bid_val[n_epi] += [bid]\n","        mae_val[n_epi] += [abs(gen - bid)]\n","        mbe_val[n_epi] += [abs(disp - bid)]\n","        rev_val[n_epi] += [revenue]\n","    \n","    # Testing Phase\n","    state = env_test.reset()  # Reset the testing environment\n","    history = (torch.zeros([n_layers, 1, hidden_size], dtype=torch.float), \n","               torch.zeros([n_layers, 1, hidden_size], dtype=torch.float))\n","\n","    # Iterate through the test data\n","    for l in range(len(env_test.data) - 1):\n","        pi_out, next_history = model.pi(torch.tensor(state[-1], dtype=torch.float), history)\n","        action = pi_out[0].tolist()  # Get action from the model\n","\n","        # Step the testing environment\n","        next_state, reward, done, info = env_test.step(action)\n","\n","        # Update state and history\n","        state = next_state[:]\n","        history = next_history\n","        \n","        # Record information for testing metrics\n","        gen = info[0]; bid = info[1]; disp = info[2]; revenue = info[3]\n","        bid_test[n_epi] += [bid]\n","        mae_test[n_epi] += [abs(gen - bid)]\n","        mbe_test[n_epi] += [abs(disp - bid)]\n","        rev_test[n_epi] += [revenue]\n","    \n","    # Print metrics at specified intervals\n","    if (n_epi + 1) % print_interval == 0:\n","        MAE_train = round(100 * np.mean(mae_train[n_epi]), 2)\n","        MAE_val   = round(100 * np.mean(mae_val[n_epi]), 2)\n","        MAE_test  = round(100 * np.mean(mae_test[n_epi]), 2)\n","        MBE_train = round(100 * np.mean(mbe_train[n_epi]), 2)\n","        MBE_val   = round(100 * np.mean(mbe_val[n_epi]), 2)\n","        MBE_test  = round(100 * np.mean(mbe_test[n_epi]), 2)\n","        REV_train = round(max_price * RE_Capacity1 * np.mean(rev_train[n_epi]), 3)\n","        REV_val   = round(max_price * RE_Capacity2 * np.mean(rev_val[n_epi]), 3)\n","        REV_test  = round(max_price * RE_Capacity3 * np.mean(rev_test[n_epi]), 3)\n","\n","        # Print the results for the current episode\n","        print(\"episode: {}\".format(n_epi + 1))\n","        print(\"MAE_train: {}%\".format(MAE_train).ljust(25), end=\"\")\n","        print(\"MAE_val: {}%\".format(MAE_val).ljust(25), end=\"\")\n","        print(\"MAE_test: {}%\".format(MAE_test).ljust(25))\n","        print(\"MBE_train: {}%\".format(MBE_train).ljust(25), end=\"\")\n","        print(\"MBE_val: {}%\".format(MBE_val).ljust(25), end=\"\")\n","        print(\"MBE_test: {}%\".format(MBE_test).ljust(25))\n","        print(\"REV_train: ${}\".format(REV_train).ljust(25), end=\"\")\n","        print(\"REV_val: ${}\".format(REV_val).ljust(25), end=\"\")\n","        print(\"REV_test: ${}\".format(REV_test).ljust(25))\n","        print(\"------------------------------------------------------------------------------------------\")\n"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"NZmiVd55l8mI"},"outputs":[],"source":["# Environment\n","\n","# Select the index of the minimum mean bias error (MBE) from the validation data (excluding the last element)\n","select_num = np.argmin(np.mean(mbe_val[:-1], axis=1))\n","\n","# Retrieve the corresponding test bids based on the selected index\n","select_test = np.array(bid_test[select_num][:])\n","\n","# Get the actual generated energy values from the test data\n","select_test_real = np.array(data_test[1:])\n","\n","# Get the prices from the test data\n","select_test_price = np.array(price_test[1:])\n","\n","# Initialize energy state at half of the maximum energy capacity\n","E = E_max / 2\n","\n","# Initialize lists to store mean bias error (MBE), rewards, and information\n","mbe = []     # List to store absolute differences between bid and dispatched energy\n","reward = []  # List to store rewards calculated during the simulation\n","info = []    # List to store detailed information about each step\n","\n","# Loop through each test bid and perform calculations\n","for i in range(len(select_test)):\n","    bid = select_test[i]         # Current bid value\n","    gen = select_test_real[i]    # Current actual generated energy\n","    imb = select_test_price[i]   # Current price\n","\n","    soc = E / E_max              # Calculate state of charge as a fraction of maximum capacity\n","\n","    # Calculate open-circuit voltage (Voc) based on state of charge (soc)\n","    Voc = a0 * np.exp(-a1 * soc) + a2 + a3 * soc - a4 * soc**2 + a5 * soc**3\n","    \n","    # Calculate series resistance (Rs) based on state of charge (soc)\n","    Rs = b0 * np.exp(-b1 * soc) + b2 + b3 * soc - b4 * soc**2 + b5 * soc**3\n","    \n","    # Calculate total resistance components\n","    Rts = c0 * np.exp(-c1 * soc) + c2\n","    Rtl = d0 * np.exp(-d1 * soc) + d2\n","    \n","    # Calculate total resistance (R)\n","    R = Rs + Rts + Rtl\n","\n","    # Calculate maximum charging current based on energy capacity and state of charge\n","    I_cmax = 1000000 * E_max * (soc_max - soc) / N / (Voc * tdelta)\n","    \n","    # Calculate maximum discharging current based on energy capacity and state of charge\n","    I_dmax = 1000000 * E_max * (soc - soc_min) / N / (Voc * tdelta)\n","    \n","    # Calculate power capacity for charging\n","    p_cmax = N * (Voc * I_cmax + I_cmax**2 * R)\n","    \n","    # Calculate power capacity for discharging\n","    p_dmax = N * (Voc * I_dmax - I_dmax**2 * R)\n","\n","    # Convert power capacities to megawatts\n","    P_cmax = p_cmax / 1000000\n","    P_dmax = p_dmax / 1000000\n","    \n","    # Calculate the actual power for charging, ensuring it's within limits\n","    P_c = min(max(gen - bid, 0), P_max, P_cmax)\n","    \n","    # Calculate the actual power for discharging, ensuring it's within limits\n","    P_d = min(max(bid - gen, 0), P_max, P_dmax)\n","    \n","    # Convert power to current (in A) for charging and discharging\n","    p_c = 1000000 * P_c / N\n","    p_d = 1000000 * P_d / N\n","\n","    # Calculate current for charging using the quadratic formula\n","    I_c = -(Voc - np.sqrt(Voc**2 + 4 * R * p_c)) / (2 * R)\n","    \n","    # Calculate current for discharging using the quadratic formula\n","    I_d = (Voc - np.sqrt(Voc**2 - 4 * R * p_d)) / (2 * R)\n","\n","    # If the charging power is not close to zero, calculate efficiency and update energy state\n","    if not np.isclose(p_c, 0):\n","        eff_c = (Voc * I_c) / p_c         # Calculate charging efficiency\n","        E = E + eff_c * P_c * tdelta       # Update energy state with charging energy\n","        disp = gen - P_c                    # Calculate dispatched energy\n","        info += [[gen, round(bid, 4), 'C', round(P_c, 4), round(disp, 4), round(eff_c, 4), round(E, 4)]]\n","    \n","    # If the discharging power is not close to zero, calculate efficiency and update energy state\n","    elif not np.isclose(p_d, 0):\n","        eff_d = p_d / (Voc * I_d)         # Calculate discharging efficiency\n","        E = E - (1 / eff_d) * P_d * tdelta  # Update energy state with discharging energy\n","        disp = gen + P_d                    # Calculate dispatched energy\n","        info += [[gen, round(bid, 4), 'D', round(P_d, 4), round(disp, 4), round(eff_d, 4), round(E, 4)]]\n","    \n","    # If neither charging nor discharging, just dispatch the generated energy\n","    else:\n","        disp = gen                          # Dispatch generated energy as is\n","        info += [[gen, round(bid, 4), 'N', 'N', round(disp, 4), 'N', round(E, 4)]]\n","    \n","    # Append the absolute difference between bid and dispatched energy to the MBE list\n","    mbe += [abs(bid - disp)]\n","    \n","    # Calculate the reward based on the dispatched energy and costs\n","    reward += [(imb * disp - imb * abs(bid - disp) - beta * (P_c + P_d)) * tdelta]\n","\n","# Calculate Mean Absolute Error (MAE) for the test data\n","MAE_test = round(100 * np.mean(np.abs(select_test_real - select_test)), 2)\n","\n","# Calculate Mean Bias Error (MBE) for the test data\n","MBE_test = round(100 * np.mean(mbe), 2)\n","\n","# Print the results of the evaluation metrics\n","print(\"MAE_test: {}%\".format(MAE_test))\n","print(\"MBE_test: {}%\".format(MBE_test))\n","print(\"REV_test: ${}\".format(round(max_price * RE_Capacity3 * np.mean(reward), 3)))\n","\n","# Optionally save the selected test bids to a CSV file\n","pd.DataFrame(select_test).to_csv(\"./Results/\"+RE+\"_Model3_DeepComp.csv\")\n"]}],"metadata":{"colab":{"authorship_tag":"ABX9TyOTNHv9EeH21Mh5rISllF8Y","provenance":[]},"kernelspec":{"display_name":"Python 3","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.11.5"}},"nbformat":4,"nbformat_minor":0}
