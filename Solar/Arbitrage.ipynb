{"cells":[{"cell_type":"code","execution_count":1,"metadata":{"id":"YbRIEKC7Ikih"},"outputs":[],"source":["import pandas as pd              # Imports the Pandas library as 'pd', commonly used for data manipulation and analysis.\n","import numpy as np               # Imports the NumPy library as 'np', which is used for numerical operations and working with arrays.\n","import torch                     # Imports the PyTorch library, which is used for building and training deep learning models.\n","import torch.nn as nn            # Imports the neural network module from PyTorch as 'nn', which contains tools for creating neural networks.\n","import torch.nn.functional as F  # Imports the functional module from PyTorch, often used for defining operations in neural networks such as activations (e.g., ReLU) and loss functions.\n"]},{"cell_type":"code","execution_count":2,"metadata":{"id":"DR-OCqAxImPV"},"outputs":[],"source":["RE = \"Solar_PBE\"  # Defines a variable 'RE' with the value \"Solar_PBE\", used as a prefix for reading specific CSV files related to solar energy data.\n","\n","# Reads training data from two CSV files for the years 2016 and 2017.\n","data_train_csv1 = pd.read_csv(\"../data/\"+RE+'_16.csv', index_col=0)  # Reads 'Solar_PBE_16.csv', setting the first column as the index.\n","data_train_csv2 = pd.read_csv(\"../data/\"+RE+'_17.csv', index_col=0)  # Reads 'Solar_PBE_17.csv', setting the first column as the index.\n","\n","# Concatenates the training data for 2016 and 2017 into one DataFrame.\n","data_train_csv  = pd.concat([data_train_csv1, data_train_csv2])   # Combines 'data_train_csv1' and 'data_train_csv2' into a single DataFrame for training.\n","\n","# Reads validation data for the year 2018 from a CSV file.\n","data_val_csv    = pd.read_csv(\"../data/\"+RE+'_18.csv', index_col=0)  # Reads 'Solar_PBE_18.csv', setting the first column as the index.\n","\n","# Reads testing data for the year 2019 from a CSV file.\n","data_test_csv   = pd.read_csv(\"../data/\"+RE+'_19.csv', index_col=0)  # Reads 'Solar_PBE_19.csv', setting the first column as the index.\n","\n","# Reads a separate CSV file containing electricity price data for the years 2016-2019.\n","data_price = pd.read_csv(\"../data/\"+'Price_Elia_Imbalance_16_19.csv', index_col=0)  # Loads the price imbalance data into a DataFrame.\n","\n","# Adds the 'Positive imbalance price' from the price data to the training data for the matching years.\n","data_train_csv['Price(€)'] = data_price['Positive imbalance price'][:len(data_train_csv)]  # Assigns prices to the training data, matching the length of the DataFrame.\n","\n","# Adds the 'Positive imbalance price' from the price data to the validation data for the corresponding year.\n","data_val_csv['Price(€)']   = data_price['Positive imbalance price'][len(data_train_csv):len(data_train_csv)+len(data_val_csv)]  # Assigns prices to the validation data, ensuring the index range matches its length.\n","\n","# Adds the 'Positive imbalance price' from the price data to the testing data for the year 2019.\n","data_test_csv['Price(€)']  = data_price['Positive imbalance price'][len(data_train_csv)+len(data_val_csv):]  # Assigns prices to the test data, covering the remaining price data after training and validation.\n","\n","# Reads predictions for training, validation, and test sets from a CSV file, and initializes with a 0.0 value at the start.\n","train_predict = [0.0] + np.array(pd.read_csv(RE+\"_Model4_DeepBid.csv\", index_col=0)).flatten().tolist()  # Reads predictions from 'Solar_PBE_Model4_DeepBid.csv', flattens them, and prepends 0.0.\n","val_predict   = [0.0] + np.array(pd.read_csv(RE+\"_Model4_DeepBid.csv\", index_col=0)).flatten().tolist()  # Same for the validation set.\n","test_predict  = [0.0] + np.array(pd.read_csv(RE+\"_Model4_DeepBid.csv\", index_col=0)).flatten().tolist()  # Same for the test set.\n"]},{"cell_type":"code","execution_count":3,"metadata":{"id":"eiXqkJByIr87"},"outputs":[],"source":["# Data Preprocessing\n","\n","Battery_Size = 0.15  # Defines the size of the battery as 0.15 p.u. (per unit), likely for energy storage considerations.\n","unit = 1  # Defines the unit of time as 1, corresponding to 15-minute intervals in the data.\n","\n","# Retrieves the maximum renewable energy (RE) power output for the training, validation, and test datasets.\n","RE_Capacity1 = max(data_train_csv['Power(MW)'])  # Maximum power output (MW) in the training data.\n","RE_Capacity2 = max(data_val_csv['Power(MW)'])    # Maximum power output (MW) in the validation data.\n","RE_Capacity3 = max(data_test_csv['Power(MW)'])   # Maximum power output (MW) in the test data.\n","\n","# Retrieves the maximum price from the price data.\n","max_price = max(data_price['Marginal incremental price'])  # Maximum marginal incremental price over the price data.\n","\n","# Calculates the number of data points for the training, validation, and test sets by dividing the length of the datasets by the time unit.\n","size_train0 = int(len(data_train_csv)/unit)  # Number of intervals in the training data based on the unit.\n","size_val0   = int(len(data_val_csv)/unit)    # Number of intervals in the validation data.\n","size_test0  = int(len(data_test_csv)/unit)   # Number of intervals in the test data.\n","\n","# Initializing lists to hold processed data and price information for training.\n","data_train0 = []; data_train = []; price_train0 = []; price_train = []\n","\n","# Loop to preprocess the training data.\n","for i in range(size_train0):\n","    # Calculates the mean power output over the 'unit' interval, normalizes it by the maximum capacity, and rounds to 3 decimal places.\n","    data_train0  += [round(pd.Series.mean(data_train_csv['Power(MW)'][i*unit:(i+1)*unit])/RE_Capacity1, 3)]\n","    # Calculates the mean price over the 'unit' interval, normalizes it by the maximum price, and rounds to 3 decimal places.\n","    price_train0 += [round(pd.Series.mean(data_train_csv['Price(€)'][i*unit:(i+1)*unit])/max_price, 3)]\n","    # If the normalized power output is greater than 0, appends it and the corresponding price to the final training data lists.\n","    if data_train0[i] > 0: \n","        data_train += [data_train0[i]]\n","        price_train += [price_train0[i]]\n","\n","# Initializing lists to hold processed data and price information for validation.\n","data_val0 = []; data_val = []; price_val0 = []; price_val = []\n","\n","# Loop to preprocess the validation data.\n","for i in range(size_val0):\n","    # Calculates the mean power output over the 'unit' interval, normalizes it by the maximum capacity, and rounds to 3 decimal places.\n","    data_val0  += [round(pd.Series.mean(data_val_csv['Power(MW)'][i*unit:(i+1)*unit])/RE_Capacity2, 3)]\n","    # Calculates the mean price over the 'unit' interval, normalizes it by the maximum price, and rounds to 3 decimal places.\n","    price_val0 += [round(pd.Series.mean(data_val_csv['Price(€)'][i*unit:(i+1)*unit])/max_price, 3)]\n","    # If the normalized power output is greater than 0, appends it and the corresponding price to the final validation data lists.\n","    if data_val0[i] > 0: \n","        data_val += [data_val0[i]]\n","        price_val += [price_val0[i]]\n","\n","# Initializing lists to hold processed data and price information for testing.\n","data_test0 = []; data_test = []; price_test0 = []; price_test = []\n","\n","# Loop to preprocess the test data.\n","for i in range(size_test0):\n","    # Calculates the mean power output over the 'unit' interval, normalizes it by the maximum capacity, and rounds to 3 decimal places.\n","    data_test0  += [round(pd.Series.mean(data_test_csv['Power(MW)'][i*unit:(i+1)*unit])/RE_Capacity3, 3)]\n","    # Calculates the mean price over the 'unit' interval, normalizes it by the maximum price, and rounds to 3 decimal places.\n","    price_test0 += [round(pd.Series.mean(data_test_csv['Price(€)'][i*unit:(i+1)*unit])/max_price, 3)]\n","    # If the normalized power output is greater than 0, appends it and the corresponding price to the final test data lists.\n","    if data_test0[i] > 0: \n","        data_test += [data_test0[i]]\n","        price_test += [price_test0[i]]\n"]},{"cell_type":"code","execution_count":4,"metadata":{"id":"Ke3I7W_aJPf_"},"outputs":[],"source":["# PPO Agent (Partially Observable State, Continuous Action Space)\n","# Assumption 1: Standard deviation is fixed\n","# Assumption 2: History is composed of observations only\n","\n","# Hyperparameters for the PPO agent\n","n_layers         = 2         # Number of LSTM layers.\n","in_size          = 3         # Input size (number of observation features).\n","hidden_size      = 64        # Number of hidden units in LSTM layers.\n","out_size         = 1         # Output size (action space dimension).\n","T_horizon        = 128       # Time horizon for batch updates.\n","learning_rate    = 0.001     # Learning rate for the optimizer.\n","K_epoch          = 3         # Number of epochs to update the policy.\n","gamma            = 0.99      # Discount factor for future rewards.\n","lmbda            = 0.95      # Smoothing factor for the Generalized Advantage Estimation (GAE).\n","eps_clip         = 0.01      # Clipping range for PPO objective.\n","C_value          = 1         # Coefficient for critic loss in the total loss function.\n","var              = 0.1**2    # Variance of the action distribution, assumed to be fixed.\n","\n","# Defining the LSTM-based neural network model for the PPO agent\n","class LSTM(nn.Module):\n","    def __init__(self):\n","        super(LSTM, self).__init__()\n","        # Linear layer to map the input size to the hidden size\n","        self.fc_s  = nn.Linear(in_size, hidden_size)\n","        # LSTM layer with `hidden_size` units and `n_layers` layers\n","        self.rnn   = nn.LSTM(hidden_size, hidden_size, n_layers, batch_first=True)\n","        # Linear layer to output the action probabilities (policy network)\n","        self.fc_pi = nn.Linear(hidden_size, out_size)\n","        # Linear layer to output the value function (critic network)\n","        self.fc_v  = nn.Linear(hidden_size, 1)\n","\n","    # Function to compute the action (policy)\n","    def pi(self, x, hidden):\n","        # Applies ReLU activation to the output of the first linear layer\n","        x = F.relu(self.fc_s(x))\n","        # Reshapes input to match LSTM input requirements (batch, seq, feature)\n","        x = x.view(1, -1, hidden_size)\n","        # Passes input through the LSTM, updating hidden state\n","        x, hidden = self.rnn(x, hidden)\n","        # Computes action probabilities using a linear layer\n","        pi = self.fc_pi(x)\n","        pi = pi.view(-1, out_size)  # Reshapes the output to match the action size\n","        return pi, hidden\n","    \n","    # Function to compute the value function (critic)\n","    def v(self, x, hidden):\n","        # Applies ReLU activation to the output of the first linear layer\n","        x = F.relu(self.fc_s(x))\n","        # Reshapes input for LSTM processing\n","        x = x.view(1, -1, hidden_size)\n","        # Passes input through the LSTM, updating hidden state\n","        x, hidden = self.rnn(x, hidden)\n","        # Computes the state value (critic output) using a linear layer\n","        v = self.fc_v(x)\n","        v = v.view(-1, 1)  # Reshapes the output to match the value size\n","        return v\n","\n","# Function to train the PPO agent using collected batches of experiences\n","def train_net(model, batch, optimizer):\n","    # Initialize lists to store different elements of the batch\n","    o, H, a, r, o_prime, H_prime, done = [], [], [], [], [], [], []\n","    \n","    # Loop through the first part of the batch (transitions)\n","    for transition in batch[0]:\n","        o.append(transition[0])       # Observation\n","        a.append(transition[1])       # Action\n","        r.append([transition[2]])     # Reward\n","        o_prime.append(transition[3]) # Next observation\n","        # Whether the episode is done (0 for false, 1 for true)\n","        done.append([0]) if transition[4] else done.append([1])\n","    \n","    # Loop through the second part of the batch (hidden states)\n","    for transition in batch[1]:\n","        H.append(transition[0])       # Hidden state at current step\n","        H_prime.append(transition[1]) # Hidden state at next step\n","        \n","    # Convert collected data into tensors for model input\n","    o         = torch.tensor(o, dtype=torch.float)    # Convert observations to tensor\n","    H         = (H[0][0].detach(), H[0][1].detach())  # Detach hidden states from computation graph\n","    a         = torch.tensor(a, dtype=torch.float)    # Convert actions to tensor\n","    r         = torch.tensor(r, dtype=torch.float)    # Convert rewards to tensor\n","    o_prime   = torch.tensor(o_prime, dtype=torch.float) # Convert next observations to tensor\n","    H_prime   = (H_prime[0][0].detach(), H_prime[0][1].detach()) # Detach next hidden states\n","    done      = torch.tensor(done)                    # Convert done flags to tensor\n","    \n","    # Calculate the old policy probability distribution using Multivariate Normal Distribution\n","    pdf_old = torch.distributions.MultivariateNormal(model.pi(o, H)[0], var*torch.eye(out_size))\n","    prob_old = torch.exp(pdf_old.log_prob(a)).view(len(a), 1)  # Old action probabilities\n","    prob_old = prob_old.detach()  # Detach from graph for stable calculations\n","    \n","    # Compute the target value (for critic)\n","    v_target = r + gamma * model.v(o_prime, H_prime) * done\n","    # Calculate temporal difference (TD) error\n","    td = r + gamma * model.v(o_prime, H_prime) * done - model.v(o, H)\n","    td = td.detach().numpy()  # Detach TD from computation graph\n","    \n","    # Generalized Advantage Estimation (GAE)\n","    advantage = []\n","    A = 0.0  # Initialize advantage to 0\n","    for delta in td[::-1].flatten():  # Traverse TD errors in reverse order\n","        A = delta + gamma * lmbda * A  # Compute advantage using smoothing factor lmbda\n","        advantage.append([A])\n","    advantage.reverse()  # Reverse to match the time step order\n","    advantage = torch.tensor(advantage, dtype=torch.float)  # Convert to tensor\n","    \n","    # Policy and Value network updates for K epochs\n","    for i in range(K_epoch):\n","        # Compute the current policy probability distribution\n","        pdf = torch.distributions.MultivariateNormal(model.pi(o, H)[0], var*torch.eye(out_size))\n","        prob = torch.exp(pdf.log_prob(a)).view(len(a), 1)  # Action probabilities under current policy\n","        \n","        # Calculate the probability ratio between current and old policies\n","        ratio = torch.exp(torch.log(prob) - torch.log(prob_old))  # ratio = prob / prob_old\n","        \n","        # Compute actor loss using clipped objective\n","        loss_actor = torch.min(ratio * advantage, torch.clamp(ratio, 1-eps_clip, 1+eps_clip) * advantage)\n","        # Compute critic loss (mean squared error between predicted and target values)\n","        loss_critic = F.mse_loss(model.v(o, H), v_target.detach())\n","        # Total loss is the negative of the actor loss minus the weighted critic loss\n","        loss = -(loss_actor - C_value * loss_critic)\n","        \n","        # Backpropagation and optimization step\n","        optimizer.zero_grad()  # Clear gradients from the previous step\n","        loss.mean().backward(retain_graph=True)  # Backpropagate the loss\n","        optimizer.step()  # Update the model parameters using the optimizer\n"]},{"cell_type":"code","execution_count":5,"metadata":{"id":"FPSKk3f7JRzp"},"outputs":[],"source":["# Define constants for the battery environment\n","E_max   = Battery_Size  # Maximum energy storage capacity of the battery\n","P_max   = E_max         # Maximum power output of the battery, equal to E_max\n","tdelta  = unit / 4      # Time interval for trading, a quarter of the unit period\n","soc_min = 0.1           # Minimum allowed State of Charge (SOC)\n","soc_max = 0.9           # Maximum allowed State of Charge (SOC)\n","\n","# Coefficients for battery voltage and resistance models\n","a0 = -1.031; a1 = 35; a2 = 3.685; a3 = 0.2156; a4 = 0.1178; a5 = 0.3201\n","b0 = 0.1463; b1 = 30.27; b2 = 0.1037; b3 = 0.0584; b4 = 0.1747; b5 = 0.1288\n","c0 = 0.1063; c1 = 62.49; c2 = 0.0437; d0 = 0.0712; d1 = 61.4; d2 = 0.0288\n","\n","N = 130 * 215 * E_max / 0.1  # A constant related to battery characteristics\n","beta = 10 / max_price         # Penalization factor for energy imbalances, scaled by the max price\n","\n","class Env():\n","    def __init__(self, data):\n","        # Initialize the environment with data\n","        self.data_gen = data[0]  # Generation data\n","        self.data_bid = data[1]   # Bid data\n","        self.data_imb = data[2]   # Imbalance data\n","        self.state = []            # Initialize state\n","\n","    def reset(self):\n","        # Reset the environment for a new episode\n","        gen = self.data_gen[0]            # Get initial generation value\n","        imb = self.data_imb[0]            # Get initial imbalance value\n","        E = E_max / 2                      # Start with half the battery's capacity\n","        state = [[gen, imb, E]]            # Set the initial state\n","        self.state = state\n","        return state                        # Return the initial state\n","\n","    def step(self, action):\n","        # Execute one time step within the environment based on the action\n","        gen = self.data_gen[len(self.state)]  # Get current generation\n","        bid = self.data_bid[len(self.state)]   # Get current bid\n","        bat = action[0]                        # Extract the battery action\n","        imb = self.data_imb[len(self.state)]   # Get current imbalance\n","\n","        E = self.state[-1][-1]                  # Get current energy state\n","        soc = E / E_max                          # Calculate SOC\n","        # Calculate battery voltage and resistance based on SOC\n","        Voc = a0 * np.exp(-a1 * soc) + a2 + a3 * soc - a4 * soc ** 2 + a5 * soc ** 3\n","        Rs = b0 * np.exp(-b1 * soc) + b2 + b3 * soc - b4 * soc ** 2 + b5 * soc ** 3\n","        Rts = c0 * np.exp(-c1 * soc) + c2\n","        Rtl = d0 * np.exp(-d1 * soc) + d2\n","        R = Rs + Rts + Rtl                      # Total resistance\n","\n","        # Calculate maximum charging and discharging currents\n","        I_cmax = 1000000 * (E_max * soc_max - E) / N / (Voc * tdelta)\n","        I_dmax = 1000000 * (E - E_max * soc_min) / N / (Voc * tdelta)\n","        # Calculate maximum power for charging and discharging\n","        p_cmax = N * (Voc * I_cmax + I_cmax ** 2 * R)\n","        p_dmax = N * (Voc * I_dmax - I_dmax ** 2 * R)\n","\n","        P_cmax = p_cmax / 1000000  # Convert to MW\n","        P_dmax = p_dmax / 1000000  # Convert to MW\n","        # Determine the actual charging and discharging power\n","        P_c = min(max(-bat * E_max, 0), P_max, P_cmax)  # Power for charging\n","        P_d = min(max(bat * E_max, 0), P_max, P_dmax)   # Power for discharging\n","        p_c = 1000000 * P_c / N  # Convert back to original units\n","        p_d = 1000000 * P_d / N  # Convert back to original units\n","\n","        # Calculate currents based on the power\n","        I_c = -(Voc - np.sqrt(Voc ** 2 + 4 * R * p_c)) / (2 * R)\n","        I_d = (Voc - np.sqrt(Voc ** 2 - 4 * R * p_d)) / (2 * R)\n","        \n","        # Determine efficiency and update energy state based on action\n","        if not np.isclose(p_c, 0):\n","            # Charging case\n","            eff_c = (Voc * I_c) / p_c  # Efficiency for charging\n","            eff_d = 1  # Assume no efficiency loss for discharging\n","            E_prime = E + eff_c * P_c * tdelta  # Update energy state\n","            disp = gen - P_c  # Determine dispatched generation\n","            bid = bid - P_c  # Update bid\n","        elif not np.isclose(p_d, 0):\n","            # Discharging case\n","            eff_d = p_d / (Voc * I_d)  # Efficiency for discharging\n","            eff_c = 1  # Assume no efficiency loss for charging\n","            E_prime = E - (1 / eff_d) * P_d * tdelta  # Update energy state\n","            disp = gen + P_d  # Determine dispatched generation\n","            bid = bid + P_d  # Update bid\n","        else:\n","            # No charging or discharging\n","            eff_c = 1  \n","            eff_d = 1\n","            E_prime = E  # Energy remains the same\n","            disp = gen  # No dispatch changes\n","\n","        # Calculate revenue based on the imbalance and dispatched generation\n","        revenue = (imb * disp - imb * abs(bid - disp) - beta * (P_c + P_d)) * tdelta\n","        \n","        next_state = self.state + [[gen, imb, E_prime]]  # Update state with new values\n","        # Calculate the reward based on the action taken\n","        reward = (imb * (P_d - P_c) - beta * (P_c + P_d) - abs(P_c - max(-bat * E_max, 0)) - abs(P_d - max(bat * E_max, 0))) * tdelta\n","        done = False  # Indicate that the episode is not done\n","        # Additional info for analysis\n","        info = [gen, bid, bat, disp, revenue]\n","\n","        self.state = next_state  # Update the environment's state\n","        return next_state, reward, done, info  # Return the next state, reward, done status, and info\n"]},{"cell_type":"code","execution_count":8,"metadata":{"id":"CGYeZlJrL_9o"},"outputs":[],"source":["# PPO Training\n","\n","total_episode = 500\n","max_iteration = int(len(data_train)/T_horizon)\n","print_interval = 1\n"," \n","model = LSTM()\n","env_train = Env([data_train, train_predict, price_train])\n","env_val   = Env([data_val, val_predict, price_val])\n","env_test  = Env([data_test, test_predict, price_test])\n","bid_train, bid_val, bid_test = [], [], [] # Bidding Value\n","bat_train, bat_val, bat_test = [], [], [] # Discharging Value\n","mae_train, mae_val, mae_test = [], [], [] # Mean Absolute Error\n","mbe_train, mbe_val, mbe_test = [], [], [] # Mean Bidding Error\n","rev_train, rev_val, rev_test = [], [], [] # Revenue"]},{"cell_type":"code","execution_count":9,"metadata":{"id":"XD7RRZ_WMMee"},"outputs":[{"name":"stdout","output_type":"stream","text":["episode: 1\n","MAE_train: 42.53%        MAE_val: 42.16%          MAE_test: 42.38%         \n","MBE_train: 42.53%        MBE_val: 42.16%          MBE_test: 42.37%         \n","REV_train: $-143.557     REV_val: $-163.324       REV_test: $-167.552      \n","------------------------------------------------------------------------------------------\n"]}],"source":["# Initialize the optimizer for the model using Adam optimizer with a specified learning rate\n","optimizer = torch.optim.Adam(model.parameters(), lr=learning_rate)\n","\n","# Loop over the number of episodes for training\n","for n_epi in range(total_episode):\n","    # Initialize lists to store bid, battery, MAE, MBE, and revenue for train, validation, and test datasets\n","    bid_train += [[]]\n","    bid_val += [[]]\n","    bid_test += [[]]\n","    bat_train += [[]]\n","    bat_val += [[]]\n","    bat_test += [[]]\n","    mae_train += [[]]\n","    mae_val += [[]]\n","    mae_test += [[]]\n","    mbe_train += [[]]\n","    mbe_val += [[]]\n","    mbe_test += [[]]\n","    rev_train += [[]]\n","    rev_val += [[]]\n","    rev_test += [[]]\n","\n","    # Reset the training environment and get the initial state\n","    state = env_train.reset()\n","    # Initialize hidden state history for the recurrent model\n","    history = (torch.zeros([n_layers, 1, hidden_size], dtype=torch.float), \n","               torch.zeros([n_layers, 1, hidden_size], dtype=torch.float))\n","\n","    # Loop for a maximum number of iterations per episode\n","    for i in range(max_iteration):\n","        batch = [[], []]  # Initialize a batch to store state-action-reward transitions\n","        for t in range(T_horizon):\n","            # Get the policy output from the model\n","            pi_out, next_history = model.pi(torch.tensor(state[-1], dtype=torch.float), history)\n","            # Sample an action from the output policy distribution\n","            action = np.random.multivariate_normal(pi_out.detach().numpy()[0], \n","                                                   var * np.identity(out_size), 1)[0].tolist()\n","            # Take a step in the environment based on the action and receive the next state and reward\n","            next_state, reward, done, info = env_train.step(action)\n","\n","            # Store the experience in the batch\n","            batch[0].append((state[-1], action, reward, next_state[-1], done))\n","            batch[1].append((history, next_history))\n","            state = next_state[:]  # Update the current state\n","            history = next_history   # Update the history\n","\n","            # Extract useful information from the environment step\n","            gen = info[0]\n","            bid = info[1]\n","            bat = info[2]\n","            disp = info[3]\n","            revenue = info[4]\n","\n","            # Collect training statistics\n","            bid_train[n_epi] += [bid]\n","            bat_train[n_epi] += [bat]\n","            mae_train[n_epi] += [abs(gen - bid)]\n","            mbe_train[n_epi] += [abs(disp - bid)]\n","            rev_train[n_epi] += [revenue]\n","\n","            # Break the loop if the episode is done\n","            if done:\n","                break\n","        \n","        # Train the model if it's not the first episode\n","        if n_epi != 0:\n","            train_net(model, batch, optimizer)\n","        if done:  # Break if the episode is done\n","            break\n","    \n","    # Validation phase\n","    state = env_val.reset()  # Reset validation environment\n","    history = (torch.zeros([n_layers, 1, hidden_size], dtype=torch.float), \n","               torch.zeros([n_layers, 1, hidden_size], dtype=torch.float))\n","\n","    # Loop over validation data\n","    for k in range(len(env_val.data_gen) - 1):\n","        pi_out, next_history = model.pi(torch.tensor(state[-1], dtype=torch.float), history)\n","        action = pi_out[0].tolist()  # Get action from policy output\n","        next_state, reward, done, info = env_val.step(action)  # Take a step in validation environment\n","\n","        # Update state and history for validation\n","        state = next_state[:]\n","        history = next_history\n","        \n","        # Extract information for validation statistics\n","        gen = info[0]\n","        bid = info[1]\n","        bat = info[2]\n","        disp = info[3]\n","        revenue = info[4]\n","        \n","        # Collect validation statistics\n","        bid_val[n_epi] += [bid]\n","        bat_val[n_epi] += [bat]\n","        mae_val[n_epi] += [abs(gen - bid)]\n","        mbe_val[n_epi] += [abs(disp - bid)]\n","        rev_val[n_epi] += [revenue]\n","    \n","    # Testing phase\n","    state = env_test.reset()  # Reset testing environment\n","    history = (torch.zeros([n_layers, 1, hidden_size], dtype=torch.float), \n","               torch.zeros([n_layers, 1, hidden_size], dtype=torch.float))\n","\n","    # Loop over test data\n","    for l in range(len(env_test.data_gen) - 1):\n","        pi_out, next_history = model.pi(torch.tensor(state[-1], dtype=torch.float), history)\n","        action = pi_out[0].tolist()  # Get action from policy output\n","        next_state, reward, done, info = env_test.step(action)  # Take a step in testing environment\n","\n","        # Update state and history for testing\n","        state = next_state[:]\n","        history = next_history\n","        \n","        # Extract information for testing statistics\n","        gen = info[0]\n","        bid = info[1]\n","        bat = info[2]\n","        disp = info[3]\n","        revenue = info[4]\n","\n","        # Collect testing statistics\n","        bid_test[n_epi] += [bid]\n","        bat_test[n_epi] += [bat]\n","        mae_test[n_epi] += [abs(gen - bid)]\n","        mbe_test[n_epi] += [abs(disp - bid)]\n","        rev_test[n_epi] += [revenue]\n","    \n","    # Print statistics at specified intervals\n","    if (n_epi + 1) % print_interval == 0:\n","        MAE_train = round(100 * np.mean(mae_train[n_epi]), 2)\n","        MAE_val = round(100 * np.mean(mae_val[n_epi]), 2)\n","        MAE_test = round(100 * np.mean(mae_test[n_epi]), 2)\n","        MBE_train = round(100 * np.mean(mbe_train[n_epi]), 2)\n","        MBE_val = round(100 * np.mean(mbe_val[n_epi]), 2)\n","        MBE_test = round(100 * np.mean(mbe_test[n_epi]), 2)\n","        REV_train = round(max_price * RE_Capacity1 * np.mean(rev_train[n_epi]), 3)\n","        REV_val = round(max_price * RE_Capacity2 * np.mean(rev_val[n_epi]), 3)\n","        REV_test = round(max_price * RE_Capacity3 * np.mean(rev_test[n_epi]), 3)\n","\n","        # Print out the results for this episode\n","        print(\"episode: {}\".format(n_epi + 1))\n","        print(\"MAE_train: {}%\".format(MAE_train).ljust(25), end=\"\")\n","        print(\"MAE_val: {}%\".format(MAE_val).ljust(25), end=\"\")\n","        print(\"MAE_test: {}%\".format(MAE_test).ljust(25))\n","        print(\"MBE_train: {}%\".format(MBE_train).ljust(25), end=\"\")\n","        print(\"MBE_val: {}%\".format(MBE_val).ljust(25), end=\"\")\n","        print(\"MBE_test: {}%\".format(MBE_test).ljust(25))\n","        print(\"REV_train: ${}\".format(REV_train).ljust(25), end=\"\")\n","        print(\"REV_val: ${}\".format(REV_val).ljust(25), end=\"\")\n","        print(\"REV_test: ${}\".format(REV_test).ljust(25))\n","        print(\"------------------------------------------------------------------------------------------\")\n"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"rdGyyoIeMQam"},"outputs":[],"source":["# Select the episode with the highest mean revenue from the validation results\n","select_num = np.argmax(np.mean(rev_val[:-1], axis=1))\n","\n","# Extract corresponding test data for the selected episode\n","select_test = np.array(bid_test[select_num][:])          # Selected bids from test\n","select_test_bat = np.array(bat_test[select_num][:])      # Selected battery outputs from test\n","select_test_real = np.array(data_test[1:])               # Actual generation data from the test set\n","select_test_price = np.array(price_test[1:])             # Prices corresponding to the test data\n","\n","# Initialize the energy state to half of the maximum capacity\n","E = E_max / 2\n","\n","# Lists to store mean bias error, rewards, and additional info during evaluation\n","mbe = []\n","reward = []\n","info = []\n","\n","# Iterate through each selected bid in the test set\n","for i in range(len(select_test)):\n","    bid = select_test[i]                     # Current bid from the model\n","    gen = select_test_real[i]                # Actual generation from the test data\n","    bat = select_test_bat[i]                 # Battery output for the current bid\n","    imb = select_test_price[i]               # Price for the current time step\n","\n","    # Calculate the state of charge (soc) as a fraction of the maximum capacity\n","    soc = E / E_max\n","\n","    # Calculate voltage (Voc) and resistance (R) based on soc using polynomial expressions\n","    Voc = a0 * np.exp(-a1 * soc) + a2 + a3 * soc - a4 * soc**2 + a5 * soc**3\n","    Rs = b0 * np.exp(-b1 * soc) + b2 + b3 * soc - b4 * soc**2 + b5 * soc**3\n","    Rts = c0 * np.exp(-c1 * soc) + c2\n","    Rtl = d0 * np.exp(-d1 * soc) + d2\n","    R = Rs + Rts + Rtl  # Total resistance\n","\n","    # Calculate maximum charge/discharge current based on state of charge and voltage\n","    I_cmax = 1000000 * E_max * (soc_max - soc) / N / (Voc * tdelta)\n","    I_dmax = 1000000 * E_max * (soc - soc_min) / N / (Voc * tdelta)\n","\n","    # Calculate maximum power for charging and discharging\n","    p_cmax = N * (Voc * I_cmax + I_cmax**2 * R)\n","    p_dmax = N * (Voc * I_dmax - I_dmax**2 * R)\n","\n","    # Calculate actual charging and discharging power based on limits\n","    P_cmax = p_cmax / 1000000  # Convert to megawatts\n","    P_dmax = p_dmax / 1000000  # Convert to megawatts\n","    P_c = min(max(-bat * E_max, 0), P_max, P_cmax)  # Charging power\n","    P_d = min(max(bat * E_max, 0), P_max, P_dmax)   # Discharging power\n","    p_c = 1000000 * P_c / N  # Convert to watts\n","    p_d = 1000000 * P_d / N  # Convert to watts\n","\n","    # Calculate currents for charging and discharging\n","    I_c = -(Voc - np.sqrt(Voc**2 + 4 * R * p_c)) / (2 * R)  # Current for charging\n","    I_d = (Voc - np.sqrt(Voc**2 - 4 * R * p_d)) / (2 * R)   # Current for discharging\n","\n","    # Evaluate charging scenario\n","    if not np.isclose(p_c, 0):\n","        eff_c = (Voc * I_c) / p_c  # Efficiency for charging\n","        E = E + eff_c * P_c * tdelta  # Update energy state after charging\n","        disp = gen - P_c  # Dispatched generation considering charging\n","        info += [[gen, round(bid, 4), 'C', round(P_c, 4), round(disp, 4), round(eff_c, 4), round(E, 4)]]\n","    # Evaluate discharging scenario\n","    elif not np.isclose(p_d, 0):\n","        eff_d = p_d / (Voc * I_d)  # Efficiency for discharging\n","        E = E - (1 / eff_d) * P_d * tdelta  # Update energy state after discharging\n","        disp = gen + P_d  # Dispatched generation considering discharging\n","        info += [[gen, round(bid, 4), 'D', round(P_d, 4), round(disp, 4), round(eff_d, 4), round(E, 4)]]\n","    # If neither charging nor discharging is possible\n","    else:\n","        disp = gen  # No change in dispatched generation\n","        info += [[gen, round(bid, 4), 'N', 'N', round(disp, 4), 'N', round(E, 4)]]\n","\n","    # Calculate the mean bias error between the bid and dispatched generation\n","    mbe += [abs(bid - disp)]\n","    # Calculate the reward based on the generated revenue and penalties\n","    reward += [(imb * disp - imb * abs(bid - disp) - beta * (P_c + P_d)) * tdelta]\n","\n","# Calculate mean absolute error (MAE) and mean bias error (MBE) for the test set\n","MAE_test = round(100 * np.mean(np.abs(select_test_real - select_test)), 2)\n","MBE_test = round(100 * np.mean(mbe), 2)\n","\n","# Print evaluation metrics\n","print(\"MAE_test: {}%\".format(MAE_test))\n","print(\"MBE_test: {}%\".format(MBE_test))\n","print(\"REV_test: ${}\".format(round(max_price * RE_Capacity3 * np.mean(reward), 3)))\n","\n","# Store the results for further analysis or saving to a file\n","result = {}\n","result['1'] = select_test_bat\n","pd.DataFrame(result).to_csv(\"./Results/\"+RE+\"_Model2_Arbitrage.csv\")\n"]}],"metadata":{"colab":{"authorship_tag":"ABX9TyO5nAEZOjdtDQBrK7ggqZi/","provenance":[]},"kernelspec":{"display_name":"Python 3","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.11.5"}},"nbformat":4,"nbformat_minor":0}
