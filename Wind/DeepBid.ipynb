{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "7602c3e7-d0ea-476c-90be-1a4521a3069d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "4873f569-75a1-4b2a-8b61-96441f00abd4",
   "metadata": {},
   "outputs": [],
   "source": [
    "RE = \"Wind_Wallonie_Elia\" \n",
    "address = \"../data/\"\n",
    "\n",
    "data_train_csv1 = pd.read_csv(address+RE+'_16.csv', index_col=0)\n",
    "data_train_csv2 = pd.read_csv(address+RE+'_17.csv', index_col=0)\n",
    "data_train_csv  = pd.concat([data_train_csv1, data_train_csv2])\n",
    "data_val_csv    = pd.read_csv(address+RE+'_18.csv', index_col=0)\n",
    "data_test_csv   = pd.read_csv(address+RE+'_19.csv', index_col=0)\n",
    "\n",
    "data_price = pd.read_csv(address+'Price_Elia_Imbalance_16_19.csv', index_col=0)\n",
    "data_train_csv['Price(€)'] = data_price['Positive imbalance price'][:len(data_train_csv)]\n",
    "data_val_csv['Price(€)']   = data_price['Positive imbalance price'][len(data_train_csv):len(data_train_csv)+len(data_val_csv)]\n",
    "data_test_csv['Price(€)']  = data_price['Positive imbalance price'][len(data_train_csv)+len(data_val_csv):]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "c1ab4328-b845-439d-b16b-017a4d682e56",
   "metadata": {},
   "outputs": [],
   "source": [
    "Battery_Size = 0.15  # p.u.  # Define the battery size as 0.15 per unit (p.u.)\n",
    "unit = 1  # Set the time unit to 1 (could represent 15 minutes, etc.)\n",
    "\n",
    "# Calculate the maximum renewable energy capacity from the training, validation, and test datasets\n",
    "RE_Capacity1 = max(data_train_csv['Power(MW)'])  # Max capacity in training data\n",
    "RE_Capacity2 = max(data_val_csv['Power(MW)'])    # Max capacity in validation data\n",
    "RE_Capacity3 = max(data_test_csv['Power(MW)'])   # Max capacity in test data\n",
    "\n",
    "# Get the maximum price from the price dataset\n",
    "max_price = max(data_price['Marginal incremental price'])  # Max price for normalizing price data\n",
    "\n",
    "# Determine the number of units in each dataset\n",
    "size_train0 = len(data_train_csv) // unit  # Number of units in training data\n",
    "size_val0 = len(data_val_csv) // unit      # Number of units in validation data\n",
    "size_test0 = len(data_test_csv) // unit    # Number of units in test data\n",
    "\n",
    "# Function to normalize power and price data\n",
    "def normalize_data(power_data, price_data, capacity, max_price, size):\n",
    "    normalized_power = []  # Initialize list for normalized power data\n",
    "    normalized_price = []   # Initialize list for normalized price data\n",
    "    \n",
    "    # Loop through each time unit\n",
    "    for i in range(size):\n",
    "        # Calculate the average power and price for the current time unit and normalize\n",
    "        power_avg = pd.Series.mean(power_data[i * unit: (i + 1) * unit]) / capacity  # Normalized power\n",
    "        price_avg = pd.Series.mean(price_data[i * unit: (i + 1) * unit]) / max_price  # Normalized price\n",
    "        \n",
    "        # Round the normalized values to 3 decimal places\n",
    "        power_avg, price_avg = round(power_avg, 3), round(price_avg, 3)\n",
    "        \n",
    "        # Only append positive normalized power values to the list\n",
    "        if power_avg > 0:\n",
    "            normalized_power.append(power_avg)  # Add to normalized power list\n",
    "            normalized_price.append(price_avg)    # Add to normalized price list\n",
    "            \n",
    "    return normalized_power, normalized_price  # Return normalized power and price lists\n",
    "\n",
    "# Normalize the training, validation, and test datasets using the defined function\n",
    "data_train, price_train = normalize_data(data_train_csv['Power(MW)'], data_train_csv['Price(€)'], RE_Capacity1, max_price, size_train0)\n",
    "data_val, price_val = normalize_data(data_val_csv['Power(MW)'], data_val_csv['Price(€)'], RE_Capacity2, max_price, size_val0)\n",
    "data_test, price_test = normalize_data(data_test_csv['Power(MW)'], data_test_csv['Price(€)'], RE_Capacity3, max_price, size_test0)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "10a4ee93",
   "metadata": {},
   "outputs": [],
   "source": [
    "n_layers         = 2  # Number of LSTM layers\n",
    "in_size          = 3  # Size of the input feature vector\n",
    "hidden_size      = 64  # Size of the hidden state in the LSTM\n",
    "out_size         = 2  # Size of the output vector (action space)\n",
    "T_horizon        = 128  # Time horizon for predictions (number of time steps)\n",
    "learning_rate    = 0.001  # Learning rate for the optimizer\n",
    "K_epoch          = 3  # Number of epochs to train the model in one update\n",
    "gamma            = 0.99  # Discount factor for future rewards\n",
    "lmbda            = 0.95  # Lambda for GAE (Generalized Advantage Estimation)\n",
    "eps_clip         = 0.01  # Epsilon for clipping in policy optimization\n",
    "C_value          = 1  # Coefficient for critic loss\n",
    "var              = 0.1**2  # Variance for action distribution\n",
    "\n",
    "# Define the LSTM neural network class\n",
    "class LSTM(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(LSTM, self).__init__()  # Initialize the parent class\n",
    "        self.fc_s  = nn.Linear(in_size, hidden_size)  # Fully connected layer from input size to hidden size\n",
    "        self.rnn   = nn.LSTM(hidden_size, hidden_size, n_layers, batch_first=True)  # LSTM layer\n",
    "        self.fc_pi = nn.Linear(hidden_size, out_size)  # Fully connected layer for policy (action probabilities)\n",
    "        self.fc_v  = nn.Linear(hidden_size, 1)  # Fully connected layer for value function (state value)\n",
    "\n",
    "    def pi(self, x, hidden):  # Policy function\n",
    "        x = F.relu(self.fc_s(x))  # Apply linear transformation and ReLU activation\n",
    "        x = x.view(1, -1, hidden_size)  # Reshape input for LSTM (sequence length, batch size, feature size)\n",
    "        x, hidden = self.rnn(x, hidden)  # Pass through LSTM\n",
    "        pi = self.fc_pi(x)  # Get action probabilities\n",
    "        pi = pi.view(-1, out_size)  # Reshape output to (batch size, output size)\n",
    "        return pi, hidden  # Return action probabilities and hidden state\n",
    "\n",
    "    def v(self, x, hidden):  # Value function\n",
    "        x = F.relu(self.fc_s(x))  # Same as pi function for state processing\n",
    "        x = x.view(1, -1, hidden_size)  # Reshape for LSTM\n",
    "        x, hidden = self.rnn(x, hidden)  # Pass through LSTM\n",
    "        v = self.fc_v(x)  # Get state value\n",
    "        v = v.view(-1, 1)  # Reshape output to (batch size, 1)\n",
    "        return v  # Return state value\n",
    "\n",
    "# Function to train the neural network\n",
    "def train_net(model, batch, optimizer):\n",
    "    # Initialize lists to store batch data\n",
    "    o, H, a, r, o_prime, H_prime, done = [], [], [], [], [], [], []\n",
    "    \n",
    "    # Extract transition data from the batch\n",
    "    for transition in batch[0]:\n",
    "        o.append(transition[0])  # State\n",
    "        a.append(transition[1])  # Action\n",
    "        r.append([transition[2]])  # Reward\n",
    "        o_prime.append(transition[3])  # Next state\n",
    "        done.append([0]) if transition[4] else done.append([1])  # Done flag\n",
    "        \n",
    "    for transition in batch[1]:\n",
    "        H.append(transition[0])  # Hidden state\n",
    "        H_prime.append(transition[1])  # Next hidden state\n",
    "\n",
    "    # Convert data to PyTorch tensors\n",
    "    o = torch.tensor(o, dtype=torch.float)  # State tensor\n",
    "    H = (H[0][0].detach(), H[0][1].detach())  # Detach hidden state for gradient tracking\n",
    "    a = torch.tensor(a, dtype=torch.float)  # Action tensor\n",
    "    r = torch.tensor(r, dtype=torch.float)  # Reward tensor\n",
    "    o_prime = torch.tensor(o_prime, dtype=torch.float)  # Next state tensor\n",
    "    H_prime = (H_prime[0][0].detach(), H_prime[0][1].detach())  # Detach next hidden state\n",
    "    done = torch.tensor(done)  # Done tensor\n",
    "\n",
    "    # Create a probability distribution for the old policy\n",
    "    pdf_old = torch.distributions.MultivariateNormal(model.pi(o, H)[0], var * torch.eye(out_size))\n",
    "    prob_old = torch.exp(pdf_old.log_prob(a)).view(len(a), 1)  # Old action probabilities\n",
    "    prob_old = prob_old.detach()  # Detach from the computational graph\n",
    "\n",
    "    # Calculate target value using rewards and next state's value\n",
    "    v_target = r + gamma * model.v(o_prime, H_prime) * done  # Target value\n",
    "    td = r + gamma * model.v(o_prime, H_prime) * done - model.v(o, H)  # Temporal difference (TD) error\n",
    "    td = td.detach().numpy()  # Detach and convert to NumPy array\n",
    "    advantage = []  # Initialize advantage list\n",
    "    A = 0.0  # Initialize advantage\n",
    "\n",
    "    # Calculate Generalized Advantage Estimation (GAE)\n",
    "    for delta in td[::-1].flatten():  # Iterate through TD errors in reverse\n",
    "        A = delta + gamma * lmbda * A  # Update advantage\n",
    "        advantage.append([A])  # Append to advantage list\n",
    "    advantage.reverse()  # Reverse to original order\n",
    "    advantage = torch.tensor(advantage, dtype=torch.float)  # Convert advantage list to tensor\n",
    "\n",
    "    # Optimize the policy and value networks\n",
    "    for i in range(K_epoch):\n",
    "        pdf = torch.distributions.MultivariateNormal(model.pi(o, H)[0], var * torch.eye(out_size))  # New policy distribution\n",
    "        prob = torch.exp(pdf.log_prob(a)).view(len(a), 1)  # New action probabilities\n",
    "        ratio = torch.exp(torch.log(prob) - torch.log(prob_old))  # Probability ratio for policy optimization\n",
    "\n",
    "        # Calculate actor loss (policy loss)\n",
    "        loss_actor = torch.min(ratio * advantage, torch.clamp(ratio, 1 - eps_clip, 1 + eps_clip) * advantage)\n",
    "        # Calculate critic loss (value loss)\n",
    "        loss_critic = F.mse_loss(model.v(o, H), v_target.detach())  # Mean squared error for value function\n",
    "        loss = -(loss_actor - C_value * loss_critic)  # Combined loss (actor-critic)\n",
    "\n",
    "        optimizer.zero_grad()  # Zero gradients for optimizer\n",
    "        loss.mean().backward(retain_graph=True)  # Backpropagate loss\n",
    "        optimizer.step()  # Update model parameters\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "4aa3c509-2dec-4ec5-b625-1b06a36a849e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Environment parameters\n",
    "E_max   = Battery_Size  # Maximum energy capacity of the battery\n",
    "P_max   = E_max         # Maximum power output, equal to the maximum energy capacity\n",
    "tdelta  = unit / 4      # Time step (e.g., 15 minutes if unit is in hours)\n",
    "soc_min = 0.1          # Minimum state of charge (SOC) for the battery\n",
    "soc_max = 0.9          # Maximum state of charge (SOC) for the battery\n",
    "\n",
    "# Coefficients for the battery performance and cost equations\n",
    "a0 = -1.031; a1 = 35; a2 = 3.685; a3 = 0.2156; a4 = 0.1178; a5 = 0.3201\n",
    "b0 = 0.1463; b1 = 30.27; b2 = 0.1037; b3 = 0.0584; b4 = 0.1747; b5 = 0.1288\n",
    "c0 = 0.1063; c1 = 62.49; c2 = 0.0437; d0 = 0.0712; d1 = 61.4; d2 = 0.0288\n",
    "\n",
    "# Total number of units or capacity in the system (adjust based on configuration)\n",
    "N = 130 * 215 * E_max / 0.1\n",
    "beta = 10 / max_price  # A scaling factor based on the maximum price\n",
    "\n",
    "class Env():\n",
    "    def __init__(self, data):\n",
    "        self.data_gen = data[0]  # Data for generation\n",
    "        self.data_imb = data[1]   # Data for imbalance prices\n",
    "        self.state = []            # Initialize the state of the environment\n",
    " \n",
    "    def reset(self):\n",
    "        # Reset the environment to the initial state\n",
    "        gen = self.data_gen[0]   # Get the first generation value\n",
    "        imb = self.data_imb[0]   # Get the first imbalance price\n",
    "        E = E_max / 2            # Initialize the energy state to half the maximum capacity\n",
    "        state = [[gen, imb, E]]  # Initialize the state with generation, imbalance, and energy\n",
    "        self.state = state        # Update the state of the environment\n",
    "        return state              # Return the initial state\n",
    " \n",
    "    def step(self, action):\n",
    "        # Execute a step in the environment based on the given action\n",
    "        gen = self.data_gen[len(self.state)]   # Get current generation value based on the state length\n",
    "        bid = action[0]                         # Bid amount from the action\n",
    "        rat = action[1]                         # Rate from the action\n",
    "        imb = self.data_imb[len(self.state)]   # Get current imbalance price based on the state length\n",
    "\n",
    "        E = self.state[-1][-1]  # Get the current energy level from the state\n",
    "        soc = E / E_max         # Calculate state of charge (SOC)\n",
    "\n",
    "        # Calculate various parameters based on SOC\n",
    "        Voc = a0 * np.exp(-a1 * soc) + a2 + a3 * soc - a4 * soc**2 + a5 * soc**3  # Open-circuit voltage\n",
    "        Rs = b0 * np.exp(-b1 * soc) + b2 + b3 * soc - b4 * soc**2 + b5 * soc**3  # Series resistance\n",
    "        Rts = c0 * np.exp(-c1 * soc) + c2  # Total resistance in the system\n",
    "        Rtl = d0 * np.exp(-d1 * soc) + d2  # Total leakage resistance\n",
    "        R = Rs + Rts + Rtl  # Combined resistance\n",
    "\n",
    "        # Calculate maximum charging and discharging current and power\n",
    "        I_cmax = 1000000 * (E_max * soc_max - E) / N / (Voc * tdelta)\n",
    "        I_dmax = 1000000 * (E - E_max * soc_min) / N / (Voc * tdelta)\n",
    "        p_cmax = N * (Voc * I_cmax + I_cmax**2 * R)  # Maximum charging power\n",
    "        p_dmax = N * (Voc * I_dmax - I_dmax**2 * R)  # Maximum discharging power\n",
    "\n",
    "        P_cmax = p_cmax / 1000000  # Convert power to MW\n",
    "        P_dmax = p_dmax / 1000000  # Convert power to MW\n",
    "\n",
    "        # Calculate actual charging and discharging power based on bid and generation\n",
    "        P_c = min(max(rat * (gen - bid), 0), P_max, P_cmax)  # Charging power\n",
    "        P_d = min(max(rat * (bid - gen), 0), P_max, P_dmax)  # Discharging power\n",
    "\n",
    "        # Calculate currents based on charging and discharging power\n",
    "        p_c = 1000000 * P_c / N  # Convert to proper scale\n",
    "        p_d = 1000000 * P_d / N  # Convert to proper scale\n",
    "\n",
    "        # Calculate charging and discharging currents using voltage and resistance\n",
    "        I_c = -(Voc - np.sqrt(Voc**2 + 4 * R * p_c)) / (2 * R)  # Charging current\n",
    "        I_d = (Voc - np.sqrt(Voc**2 - 4 * R * p_d)) / (2 * R)    # Discharging current\n",
    "        \n",
    "        # Update the energy state based on charging/discharging\n",
    "        if not np.isclose(p_c, 0):  # If charging power is not zero\n",
    "            eff_c = (Voc * I_c) / p_c  # Calculate charging efficiency\n",
    "            eff_d = 1                   # Assume discharging efficiency is 1\n",
    "            E_prime = E + eff_c * P_c * tdelta  # Update energy state after charging\n",
    "            disp = gen - P_c            # Calculate dispatched generation\n",
    "        elif not np.isclose(p_d, 0):  # If discharging power is not zero\n",
    "            eff_d = p_d / (Voc * I_d)  # Calculate discharging efficiency\n",
    "            eff_c = 1                   # Assume charging efficiency is 1\n",
    "            E_prime = E - (1 / eff_d) * P_d * tdelta  # Update energy state after discharging\n",
    "            disp = gen + P_d            # Calculate dispatched generation\n",
    "        else:  # If neither charging nor discharging\n",
    "            eff_c = 1; eff_d = 1  # Assume efficiencies are 1\n",
    "            E_prime = E            # Energy state remains unchanged\n",
    "            disp = gen             # Dispatch generation remains the same\n",
    "\n",
    "        # Calculate revenue based on imbalance, dispatched generation, and costs\n",
    "        revenue = (imb * disp - imb * abs(bid - disp) - beta * (P_c + P_d)) * tdelta\n",
    "\n",
    "        # Update the next state with current generation, imbalance, and new energy level\n",
    "        next_state = self.state + [[gen, imb, E_prime]]\n",
    "        reward = revenue - imb * gen * tdelta  # Calculate reward\n",
    "        done = False  # Environment is not done yet\n",
    "        info = [gen, bid, rat, disp, revenue]  # Additional information for debugging\n",
    " \n",
    "        self.state = next_state  # Update the state of the environment\n",
    "        return next_state, reward, done, info  # Return the next state, reward, done flag, and info\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "2b1ee53f-39fa-43fd-a3e3-1c9e2388b62f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# PPO Training\n",
    "\n",
    "total_episode = 100\n",
    "max_iteration = int(len(data_train)/T_horizon)\n",
    "print_interval = 1\n",
    " \n",
    "model = LSTM()\n",
    "env_train = Env([data_train, price_train])\n",
    "env_val   = Env([data_val, price_val])\n",
    "env_test  = Env([data_test, price_test])\n",
    "bid_train, bid_val, bid_test = [], [], [] # Bidding Value\n",
    "rat_train, rat_val, rat_test = [], [], [] # Compensation Ratio\n",
    "mae_train, mae_val, mae_test = [], [], [] # Mean Absolute Error\n",
    "mbe_train, mbe_val, mbe_test = [], [], [] # Mean Bidding Error\n",
    "rev_train, rev_val, rev_test = [], [], [] # Revenue"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "dcf107ac-b70c-4974-b012-f8c347bac0a6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "episode: 1\n",
      "MAE_train: 31.34%        MAE_val: 33.69%          MAE_test: 30.35%         \n",
      "MBE_train: 31.26%        MBE_val: 33.69%          MBE_test: 30.35%         \n",
      "REV_train: $-25.854      REV_val: $-21.404        REV_test: $-18.209       \n",
      "------------------------------------------------------------------------------------------\n",
      "episode: 2\n",
      "MAE_train: 28.12%        MAE_val: 27.12%          MAE_test: 23.92%         \n",
      "MBE_train: 27.94%        MBE_val: 27.09%          MBE_test: 23.89%         \n",
      "REV_train: $-6.638       REV_val: $35.064         REV_test: $26.832        \n",
      "------------------------------------------------------------------------------------------\n",
      "episode: 3\n",
      "MAE_train: 23.35%        MAE_val: 22.41%          MAE_test: 19.43%         \n",
      "MBE_train: 22.96%        MBE_val: 22.13%          MBE_test: 19.14%         \n",
      "REV_train: $18.104       REV_val: $72.695         REV_test: $54.3          \n",
      "------------------------------------------------------------------------------------------\n",
      "episode: 4\n",
      "MAE_train: 22.39%        MAE_val: 22.9%           MAE_test: 19.85%         \n",
      "MBE_train: 21.98%        MBE_val: 22.71%          MBE_test: 19.65%         \n",
      "REV_train: $23.076       REV_val: $69.117         REV_test: $52.398        \n",
      "------------------------------------------------------------------------------------------\n",
      "episode: 5\n",
      "MAE_train: 19.85%        MAE_val: 21.53%          MAE_test: 18.71%         \n",
      "MBE_train: 19.38%        MBE_val: 21.21%          MBE_test: 18.36%         \n",
      "REV_train: $33.684       REV_val: $78.199         REV_test: $56.866        \n",
      "------------------------------------------------------------------------------------------\n",
      "episode: 6\n",
      "MAE_train: 19.72%        MAE_val: 19.38%          MAE_test: 16.5%          \n",
      "MBE_train: 19.34%        MBE_val: 19.24%          MBE_test: 16.35%         \n",
      "REV_train: $33.703       REV_val: $93.637         REV_test: $71.14         \n",
      "------------------------------------------------------------------------------------------\n",
      "episode: 7\n",
      "MAE_train: 17.8%         MAE_val: 16.5%           MAE_test: 13.76%         \n",
      "MBE_train: 17.35%        MBE_val: 16.24%          MBE_test: 13.48%         \n",
      "REV_train: $42.784       REV_val: $114.521        REV_test: $85.042        \n",
      "------------------------------------------------------------------------------------------\n",
      "episode: 8\n",
      "MAE_train: 16.72%        MAE_val: 16.28%          MAE_test: 13.57%         \n",
      "MBE_train: 16.17%        MBE_val: 15.97%          MBE_test: 13.25%         \n",
      "REV_train: $47.701       REV_val: $116.504        REV_test: $86.052        \n",
      "------------------------------------------------------------------------------------------\n",
      "episode: 9\n",
      "MAE_train: 14.81%        MAE_val: 12.05%          MAE_test: 9.55%          \n",
      "MBE_train: 14.23%        MBE_val: 11.87%          MBE_test: 9.37%          \n",
      "REV_train: $54.871       REV_val: $144.354        REV_test: $108.245       \n",
      "------------------------------------------------------------------------------------------\n",
      "episode: 10\n",
      "MAE_train: 12.16%        MAE_val: 10.21%          MAE_test: 8.1%           \n",
      "MBE_train: 11.35%        MBE_val: 9.86%           MBE_test: 7.74%          \n",
      "REV_train: $67.55        REV_val: $157.757        REV_test: $114.374       \n",
      "------------------------------------------------------------------------------------------\n",
      "episode: 11\n",
      "MAE_train: 11.62%        MAE_val: 8.86%           MAE_test: 6.92%          \n",
      "MBE_train: 10.54%        MBE_val: 8.59%           MBE_test: 6.63%          \n",
      "REV_train: $70.69        REV_val: $167.622        REV_test: $121.16        \n",
      "------------------------------------------------------------------------------------------\n",
      "episode: 12\n",
      "MAE_train: 11.21%        MAE_val: 8.67%           MAE_test: 6.81%          \n",
      "MBE_train: 10.01%        MBE_val: 8.29%           MBE_test: 6.42%          \n",
      "REV_train: $71.088       REV_val: $168.326        REV_test: $120.309       \n",
      "------------------------------------------------------------------------------------------\n",
      "episode: 13\n",
      "MAE_train: 9.89%         MAE_val: 7.0%            MAE_test: 5.72%          \n",
      "MBE_train: 8.68%         MBE_val: 6.54%           MBE_test: 5.25%          \n",
      "REV_train: $76.742       REV_val: $176.848        REV_test: $122.411       \n",
      "------------------------------------------------------------------------------------------\n",
      "episode: 14\n",
      "MAE_train: 9.96%         MAE_val: 5.26%           MAE_test: 4.66%          \n",
      "MBE_train: 8.65%         MBE_val: 4.84%           MBE_test: 4.37%          \n",
      "REV_train: $76.645       REV_val: $181.913        REV_test: $121.013       \n",
      "------------------------------------------------------------------------------------------\n",
      "episode: 15\n",
      "MAE_train: 9.6%          MAE_val: 6.47%           MAE_test: 5.31%          \n",
      "MBE_train: 8.27%         MBE_val: 6.42%           MBE_test: 5.26%          \n",
      "REV_train: $78.59        REV_val: $177.243        REV_test: $123.506       \n",
      "------------------------------------------------------------------------------------------\n",
      "episode: 16\n",
      "MAE_train: 9.5%          MAE_val: 5.42%           MAE_test: 4.22%          \n",
      "MBE_train: 8.29%         MBE_val: 5.08%           MBE_test: 3.88%          \n",
      "REV_train: $77.306       REV_val: $187.355        REV_test: $130.892       \n",
      "------------------------------------------------------------------------------------------\n",
      "episode: 17\n",
      "MAE_train: 9.14%         MAE_val: 4.52%           MAE_test: 4.08%          \n",
      "MBE_train: 7.95%         MBE_val: 4.29%           MBE_test: 3.95%          \n",
      "REV_train: $76.916       REV_val: $181.431        REV_test: $121.002       \n",
      "------------------------------------------------------------------------------------------\n",
      "episode: 18\n",
      "MAE_train: 8.7%          MAE_val: 3.83%           MAE_test: 3.08%          \n",
      "MBE_train: 7.49%         MBE_val: 3.55%           MBE_test: 2.84%          \n",
      "REV_train: $79.539       REV_val: $195.311        REV_test: $134.528       \n",
      "------------------------------------------------------------------------------------------\n",
      "episode: 19\n",
      "MAE_train: 8.59%         MAE_val: 4.74%           MAE_test: 4.43%          \n",
      "MBE_train: 7.34%         MBE_val: 4.69%           MBE_test: 4.39%          \n",
      "REV_train: $80.756       REV_val: $179.877        REV_test: $118.808       \n",
      "------------------------------------------------------------------------------------------\n",
      "episode: 20\n",
      "MAE_train: 8.68%         MAE_val: 3.68%           MAE_test: 2.92%          \n",
      "MBE_train: 7.41%         MBE_val: 3.36%           MBE_test: 2.59%          \n",
      "REV_train: $81.146       REV_val: $199.215        REV_test: $138.385       \n",
      "------------------------------------------------------------------------------------------\n",
      "episode: 21\n",
      "MAE_train: 8.62%         MAE_val: 5.03%           MAE_test: 3.98%          \n",
      "MBE_train: 7.08%         MBE_val: 4.81%           MBE_test: 3.74%          \n",
      "REV_train: $82.458       REV_val: $191.608        REV_test: $134.733       \n",
      "------------------------------------------------------------------------------------------\n",
      "episode: 22\n",
      "MAE_train: 9.99%         MAE_val: 5.26%           MAE_test: 4.24%          \n",
      "MBE_train: 8.52%         MBE_val: 5.03%           MBE_test: 4.05%          \n",
      "REV_train: $79.568       REV_val: $190.475        REV_test: $133.677       \n",
      "------------------------------------------------------------------------------------------\n",
      "episode: 23\n",
      "MAE_train: 9.72%         MAE_val: 4.36%           MAE_test: 3.64%          \n",
      "MBE_train: 8.13%         MBE_val: 3.8%            MBE_test: 3.19%          \n",
      "REV_train: $80.42        REV_val: $190.7          REV_test: $130.171       \n",
      "------------------------------------------------------------------------------------------\n",
      "episode: 24\n",
      "MAE_train: 8.76%         MAE_val: 2.62%           MAE_test: 2.42%          \n",
      "MBE_train: 7.12%         MBE_val: 2.38%           MBE_test: 2.19%          \n",
      "REV_train: $82.129       REV_val: $198.766        REV_test: $134.543       \n",
      "------------------------------------------------------------------------------------------\n",
      "episode: 25\n",
      "MAE_train: 8.56%         MAE_val: 4.53%           MAE_test: 4.45%          \n",
      "MBE_train: 6.94%         MBE_val: 4.49%           MBE_test: 4.4%           \n",
      "REV_train: $82.667       REV_val: $177.483        REV_test: $116.276       \n",
      "------------------------------------------------------------------------------------------\n",
      "episode: 26\n",
      "MAE_train: 8.39%         MAE_val: 2.09%           MAE_test: 1.93%          \n",
      "MBE_train: 6.3%          MBE_val: 1.48%           MBE_test: 1.34%          \n",
      "REV_train: $85.821       REV_val: $207.252        REV_test: $142.02        \n",
      "------------------------------------------------------------------------------------------\n",
      "episode: 27\n",
      "MAE_train: 8.43%         MAE_val: 3.28%           MAE_test: 2.92%          \n",
      "MBE_train: 5.93%         MBE_val: 2.7%            MBE_test: 2.45%          \n",
      "REV_train: $87.09        REV_val: $193.791        REV_test: $131.597       \n",
      "------------------------------------------------------------------------------------------\n",
      "episode: 28\n",
      "MAE_train: 8.46%         MAE_val: 4.9%            MAE_test: 4.86%          \n",
      "MBE_train: 6.12%         MBE_val: 4.88%           MBE_test: 4.83%          \n",
      "REV_train: $84.793       REV_val: $170.818        REV_test: $111.531       \n",
      "------------------------------------------------------------------------------------------\n"
     ]
    }
   ],
   "source": [
    "# Set up the optimizer for the model using Adam with the specified learning rate\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=learning_rate)\n",
    "\n",
    "# Iterate over the total number of episodes for training\n",
    "for n_epi in range(total_episode):\n",
    "    # Initialize lists to store bids, ratios, and revenue metrics for training, validation, and testing\n",
    "    bid_train += [[]]; bid_val += [[]]; bid_test += [[]]\n",
    "    rat_train += [[]]; rat_val += [[]]; rat_test += [[]]\n",
    "    mae_train += [[]]; mae_val += [[]]; mae_test += [[]]\n",
    "    mbe_train += [[]]; mbe_val += [[]]; mbe_test += [[]]\n",
    "    rev_train += [[]]; rev_val += [[]]; rev_test += [[]]\n",
    " \n",
    "    # Reset the training environment to start a new episode\n",
    "    state = env_train.reset()\n",
    "    # Initialize the hidden state for the recurrent model\n",
    "    history = (torch.zeros([n_layers, 1, hidden_size], dtype=torch.float), \n",
    "               torch.zeros([n_layers, 1, hidden_size], dtype=torch.float))\n",
    "\n",
    "    # Iterate over the maximum number of iterations within an episode\n",
    "    for i in range(max_iteration):\n",
    "        # Initialize a batch to store transitions\n",
    "        batch = [[],[]]\n",
    "        \n",
    "        # Sample actions for T_horizon timesteps\n",
    "        for t in range(T_horizon):\n",
    "            # Get the action probabilities from the policy network\n",
    "            pi_out, next_history = model.pi(torch.tensor(state[-1], dtype=torch.float), history)\n",
    "            # Sample an action from the multivariate normal distribution\n",
    "            action = np.random.multivariate_normal(pi_out.detach().numpy()[0], var*np.identity(out_size), 1)[0].tolist()\n",
    "            # Take a step in the environment using the sampled action\n",
    "            next_state, reward, done, info = env_train.step(action)\n",
    " \n",
    "            # Store the transition in the batch\n",
    "            batch[0].append((state[-1], action, reward, next_state[-1], done))\n",
    "            batch[1].append((history, next_history))\n",
    "            # Update the current state and history\n",
    "            state = next_state[:]\n",
    "            history = next_history\n",
    " \n",
    "            # Unpack information from the environment step\n",
    "            gen = info[0]; bid = info[1]; rat = info[2]; disp = info[3]; revenue = info[4]\n",
    "            # Collect training data\n",
    "            bid_train[n_epi] += [bid]\n",
    "            rat_train[n_epi] += [rat]\n",
    "            mae_train[n_epi] += [abs(gen - bid)]\n",
    "            mbe_train[n_epi] += [abs(disp - bid)]\n",
    "            rev_train[n_epi] += [revenue]\n",
    "            # Break if the episode is done\n",
    "            if done:\n",
    "                break\n",
    "        \n",
    "        # Train the model if this is not the first episode\n",
    "        if n_epi != 0:\n",
    "            train_net(model, batch, optimizer)\n",
    "        # Break if the episode is done\n",
    "        if done:\n",
    "            break\n",
    "    \n",
    "    # Validate the model using the validation environment\n",
    "    state = env_val.reset()\n",
    "    history = (torch.zeros([n_layers, 1, hidden_size], dtype=torch.float), \n",
    "               torch.zeros([n_layers, 1, hidden_size], dtype=torch.float))\n",
    "    for k in range(len(env_val.data_gen)-1):\n",
    "        # Get action probabilities from the model\n",
    "        pi_out, next_history = model.pi(torch.tensor(state[-1], dtype=torch.float), history)\n",
    "        action = pi_out[0].tolist()  # Choose the action from the output\n",
    "        next_state, reward, done, info = env_val.step(action)  # Step in the environment\n",
    " \n",
    "        # Update state and history for the next iteration\n",
    "        state = next_state[:]\n",
    "        history = next_history\n",
    "        \n",
    "        # Unpack information from the environment step\n",
    "        gen = info[0]; bid = info[1]; rat = info[2]; disp = info[3]; revenue = info[4]\n",
    "        # Collect validation data\n",
    "        bid_val[n_epi] += [bid]\n",
    "        rat_val[n_epi] += [rat]\n",
    "        mae_val[n_epi] += [abs(gen - bid)]\n",
    "        mbe_val[n_epi] += [abs(disp - bid)]\n",
    "        rev_val[n_epi] += [revenue]\n",
    "    \n",
    "    # Test the model using the test environment\n",
    "    state = env_test.reset()\n",
    "    history = (torch.zeros([n_layers, 1, hidden_size], dtype=torch.float), \n",
    "               torch.zeros([n_layers, 1, hidden_size], dtype=torch.float))\n",
    "    for l in range(len(env_test.data_gen)-1):\n",
    "        # Get action probabilities from the model\n",
    "        pi_out, next_history = model.pi(torch.tensor(state[-1], dtype=torch.float), history)\n",
    "        action = pi_out[0].tolist()  # Choose the action from the output\n",
    "        next_state, reward, done, info = env_test.step(action)  # Step in the environment\n",
    " \n",
    "        # Update state and history for the next iteration\n",
    "        state = next_state[:]\n",
    "        history = next_history\n",
    "        \n",
    "        # Unpack information from the environment step\n",
    "        gen = info[0]; bid = info[1]; rat = info[2]; disp = info[3]; revenue = info[4]\n",
    "        # Collect test data\n",
    "        bid_test[n_epi] += [bid]\n",
    "        rat_test[n_epi] += [rat]\n",
    "        mae_test[n_epi] += [abs(gen - bid)]\n",
    "        mbe_test[n_epi] += [abs(disp - bid)]\n",
    "        rev_test[n_epi] += [revenue]\n",
    "    \n",
    "    # Print metrics every 'print_interval' episodes\n",
    "    if (n_epi+1)%print_interval == 0:\n",
    "        MAE_train = round(100*np.mean(mae_train[n_epi]), 2)  # Mean Absolute Error for training\n",
    "        MAE_val   = round(100*np.mean(mae_val[n_epi]), 2)    # Mean Absolute Error for validation\n",
    "        MAE_test  = round(100*np.mean(mae_test[n_epi]), 2)   # Mean Absolute Error for testing\n",
    "        MBE_train = round(100*np.mean(mbe_train[n_epi]), 2)   # Mean Bias Error for training\n",
    "        MBE_val   = round(100*np.mean(mbe_val[n_epi]), 2)     # Mean Bias Error for validation\n",
    "        MBE_test  = round(100*np.mean(mbe_test[n_epi]), 2)    # Mean Bias Error for testing\n",
    "        REV_train = round(max_price * RE_Capacity1 * np.mean(rev_train[n_epi]), 3)  # Revenue for training\n",
    "        REV_val   = round(max_price * RE_Capacity2 * np.mean(rev_val[n_epi]), 3)    # Revenue for validation\n",
    "        REV_test  = round(max_price * RE_Capacity3 * np.mean(rev_test[n_epi]), 3)   # Revenue for testing\n",
    " \n",
    "        # Print the results for the current episode\n",
    "        print(\"episode: {}\".format(n_epi+1))\n",
    "        print(\"MAE_train: {}%\".format(MAE_train).ljust(25), end=\"\")\n",
    "        print(\"MAE_val: {}%\".format(MAE_val).ljust(25), end=\"\")\n",
    "        print(\"MAE_test: {}%\".format(MAE_test).ljust(25))\n",
    "        print(\"MBE_train: {}%\".format(MBE_train).ljust(25), end=\"\")\n",
    "        print(\"MBE_val: {}%\".format(MBE_val).ljust(25), end=\"\")\n",
    "        print(\"MBE_test: {}%\".format(MBE_test).ljust(25))\n",
    "        print(\"REV_train: ${}\".format(REV_train).ljust(25), end=\"\")\n",
    "        print(\"REV_val: ${}\".format(REV_val).ljust(25), end=\"\")\n",
    "        print(\"REV_test: ${}\".format(REV_test).ljust(25))\n",
    "        print(\"------------------------------------------------------------------------------------------\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "acd415db-de57-4b33-a185-e6dbdbd7d91f",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Environment\n",
    " \n",
    "select_num = np.argmax(np.mean(rev_val[:-1],axis=1))\n",
    "select_test = np.array(bid_test[select_num][:])\n",
    "select_test_rat = np.array(rat_test[select_num][:])\n",
    "select_test_real = np.array(data_test[1:])\n",
    "select_test_price = np.array(price_test[1:])\n",
    " \n",
    "E = E_max/2\n",
    "mbe = []\n",
    "reward = []\n",
    "info = []\n",
    "for i in range(len(select_test)):\n",
    "    bid = select_test[i]\n",
    "    gen = select_test_real[i]\n",
    "    rat = select_test_rat[i]\n",
    "    imb = select_test_price[i]\n",
    "    \n",
    "    soc = E/E_max\n",
    "    Voc = a0*np.exp(-a1*soc) + a2 + a3*soc - a4*soc**2 + a5*soc**3\n",
    "    Rs  = b0*np.exp(-b1*soc) + b2 + b3*soc - b4*soc**2 + b5*soc**3\n",
    "    Rts = c0*np.exp(-c1*soc) + c2\n",
    "    Rtl = d0*np.exp(-d1*soc) + d2\n",
    "    R   = Rs + Rts + Rtl\n",
    " \n",
    "    I_cmax = 1000000*E_max*(soc_max - soc)/N/(Voc*tdelta)\n",
    "    I_dmax = 1000000*E_max*(soc - soc_min)/N/(Voc*tdelta)\n",
    "    p_cmax = N*(Voc*I_cmax + I_cmax**2*R)\n",
    "    p_dmax = N*(Voc*I_dmax - I_dmax**2*R)\n",
    " \n",
    "    P_cmax = p_cmax/1000000; P_dmax = p_dmax/1000000\n",
    "    P_c = min(max(rat*(gen-bid), 0), P_max, P_cmax)\n",
    "    P_d = min(max(rat*(bid-gen), 0), P_max, P_dmax)\n",
    "    p_c = 1000000*P_c/N; p_d = 1000000*P_d/N\n",
    " \n",
    "    I_c = -(Voc - np.sqrt(Voc**2 + 4*R*p_c))/(2*R)\n",
    "    I_d = (Voc - np.sqrt(Voc**2 - 4*R*p_d))/(2*R)\n",
    "    if not np.isclose(p_c, 0):\n",
    "        eff_c = (Voc*I_c)/p_c\n",
    "        E = E + eff_c*P_c*tdelta\n",
    "        disp = gen - P_c\n",
    "        info += [[gen, round(bid,4), 'C', round(P_c,4), round(disp,4), round(eff_c,4), round(E,4)]]\n",
    "    elif not np.isclose(p_d, 0):\n",
    "        eff_d = p_d/(Voc*I_d)\n",
    "        E = E - (1/eff_d)*P_d*tdelta\n",
    "        disp = gen + P_d\n",
    "        info += [[gen, round(bid,4), 'D', round(P_d,4), round(disp,4), round(eff_d,4), round(E,4)]]\n",
    "    else:\n",
    "        disp = gen\n",
    "        info += [[gen, round(bid,4), 'N', 'N', round(disp,4), 'N', round(E,4)]]\n",
    "    \n",
    "    mbe += [abs(bid - disp)]\n",
    "    reward += [(imb*disp - imb*abs(bid-disp) - beta*(P_c+P_d))*tdelta]\n",
    " \n",
    "MAE_test = round(100*np.mean(np.abs(select_test_real - select_test)),2)\n",
    "MBE_test = round(100*np.mean(mbe),2)\n",
    "print(\"MAE_test: {}%\".format(MAE_test))\n",
    "print(\"MBE_test: {}%\".format(MBE_test))\n",
    "print(\"REV_test: ${}\".format(round(max_price*RE_Capacity3*np.mean(reward),3)))\n",
    "\n",
    "result = {}\n",
    "result['0'] = select_test\n",
    "result['1'] = select_test_rat\n",
    "\n",
    "pd.DataFrame(result).to_csv(\"./Results/\"+RE+\"_Model4_DeepBid.csv\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
